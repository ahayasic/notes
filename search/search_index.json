{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Study notes","text":"<p>This is a collection of study notes.</p>"},{"location":"apache-spark/","title":"Apache Spark","text":"<p>Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing.</p>"},{"location":"apache-spark/structured-streaming/","title":"Structured Streaming","text":"<p>The Apache Spark Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine [1]. By extending the Spark SQL APIs to support streaming workloads, developers can express the streaming computation the same way they would express a batch computation on static data in all the supported language bindings for Spark SQL and including streaming aggregations, event-time windows, stream-to-batch joins, while inherits the underlying optimizations of Spark SQL, in special the use of the Catalyst query optimizer and the low overhead memory management and code generation delivered by Project Tungsten.</p> <p>Not only The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive following the micro-batch processing model as well system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs. In short,\u00a0Structured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming.</p>"},{"location":"apache-spark/structured-streaming/#topics-to-write-about","title":"Topics to write about","text":"<ul> <li> Incremental execution. How it works?</li> <li> It\u2019s possible to change the streaming query once it\u2019s started?</li> <li> How delivery semantics works? What\u2019s the relationship with source, sink and checkpoints?</li> <li>[ ]</li> </ul>"},{"location":"apache-spark/structured-streaming/#references","title":"References","text":"<p>[1] Apache Spark Structured Streaming Official Documentation.</p> <p>[2]</p>"},{"location":"apache-spark/structured-streaming/event-time-based-processing/","title":"Event Time-Based Processing","text":"<p>When analyzing patterns in event data over time, like aggregating data into specific time windows, it's essential to process the events as if we were observing them at the moment they were generated. Event-time processing refers to looking at the stream of events from the timeline at which they were produced and applying the processing logic from that perspective.</p> <p>For instance, consider a temperature sensor that sends data every minute. To detect if any sensor is reporting a number higher than a threshold, we can check the max temperature received from each sensor in five-minute intervals.</p> <p>To do this, the device or system that generates the event needs to \"stamp\" the events with the time of creation. Structured Streaming uses the timestamp field in events to maintain a monotonically increasing upper bound, forming a nonlinear event timeline that guides the event-time processing. This timeline is used to handle with time-based features in Structured Streaming, such as windowed aggregations.</p> <p>The ability of Structured Streaming to understand the time flow of the event source decouples event generation from processing time, allowing for the replay of a sequence of past events and have Structured Streaming produce the correct results for all event-time aggregations (backfilling).</p> <p>Nonlinear event timeline</p> <p>The timeline in Spark is effectively formed by the event-time timestamps. The timeline is not a simple sequence of events but a representation of the event-time of the events. Spark maintains a monotonically increasing upper bound on this timeline, ensuring that out-of-order events that fall within the watermark delay are still considered for processing. Due to the possibility of out-of-order events, the timeline is nonlinear. Events do not strictly follow real-time but are aligned according to their event-time timestamps. This nonlinear timeline allows Spark to process events based on when they occurred, rather than when they are processed.</p>"},{"location":"apache-spark/structured-streaming/event-time-based-processing/#window-operations-on-event-time","title":"Window Operations on Event Time","text":"<p>Aggregations over a sliding event-time window are straightforward with Structured Streaming and are very similar to grouped aggregations. In a grouped aggregation, aggregate values (e.g. counts) are maintained for each unique value in the user-specified grouping column. In case of window-based aggregations, aggregate values are maintained for each window the event-time of a row falls into.</p>"},{"location":"apache-spark/structured-streaming/event-time-based-processing/#watermarks","title":"Watermarks","text":"<p>External factors can affect the deliv\u2010 ery of event messages and, hence, when using event time for processing, we didn\u2019t have a guarantee of order or delivery. Events might be late or never arrive at all. So, how late is too late? For how long do we hold partial aggregations before considering them complete?</p> <p>A watermark is a time threshold that dictates how long we wait for events before declaring that they are too late. Events that are considered late beyond the watermark are discarded. Watermarks are computed as a threshold based on the \"internal time representation\". The watermark line is a shifted line from the event-time timeline inferred from the event\u2019s time information. The watermark is, at any given moment, the oldest timestamp that we will accept on the data stream. Any events that are older than this expectation are not taken into the results of the stream processing. The streaming engine can choose to process them in an alternative way, like report them in a late arrivals channel, for example.</p> <p>To account for possibly delayed events, we use the concept called watermark. A watermark is usually much larger than the average delay we expect in the delivery of the events. Note also that this watermark is a fluid value that monotonically increases over time,2 sliding a win\u2010 dow of delay-tolerance as the time observed in the data-stream progresses.</p> <p>When we apply this concept of watermark to our event-time diagram, as illustrated in Figure 2-7, we can appreciate that the watermark closes the open boundary left by the definition of event-time window, providing criteria to decide what events belong to the window, and what events are too late to be considered for processing.</p>"},{"location":"apache-spark/structured-streaming/event-time-based-processing/#questions","title":"Questions","text":"<ul> <li>How the output mode influences the event-time processing?</li> <li>How backfilling is done in Structured Streaming?</li> </ul>"},{"location":"apache-spark/structured-streaming/programming-model/","title":"Programming Model","text":"<p>The key idea in Structured Streaming is to treat a live data stream as a table that is being continuously appended. This leads to a new stream processing model that is very similar to a batch processing model. You will express your streaming computation as standard batch-like query as on a static table, and Spark runs it as an\u00a0incremental\u00a0query on the\u00a0unbounded\u00a0input table.</p> <p>Considering the structure defined by Learning of Spark, the Structured Streaming Programming Model has five main steps:</p> <ul> <li>Define the source</li> <li>Transform the data</li> <li>Define output sink and output mode</li> <li>Set processing details</li> <li>Start the query</li> </ul> <p>Here is a brief breakdown of each step:</p>"},{"location":"apache-spark/structured-streaming/programming-model/#define-the-source","title":"Define the source","text":"<p>Similar to batch data processing, the initial step in streaming is to establish a DataFrame from a data source. In contrast to batch processing, however, reading from a data stream source utilizes <code>spark.readStream</code>, which incorporates a <code>DataStreamReader</code>.</p> <p>This reader functions similarly to the <code>DataFrameReader</code> used in <code>spark.read</code> with a few differences. The <code>format</code> method is used to specify the streaming source, and the <code>options</code> method is used to set source-specific options.</p> <p>Once the source is defined, the <code>load</code> method is called to create a streaming DataFrame that represents the data stream.</p> <pre><code>from pyspark.sql.types import StructType, StructField, StringType\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import SparkSession\n\nschema = StructType([\n    StructField(\"lines\", StringType(), True)\n])\n\nspark = SparkSession.builder.master(\"local[*]\").appName(\"WordsCount\").getOrCreate()\n\nlines = (\n    spark\n    .readStream\n    .format(\"csv\")\n    .schema(schema)\n    .option(\"header\", \"false\")\n    .load(\"/structured-streaming-basics/data/readStream/\")\n)\n</code></pre>"},{"location":"apache-spark/structured-streaming/programming-model/#transform-the-data","title":"Transform the data","text":"<p>The <code>DataStreamReader</code> returns a <code>DataStream</code> data structure, which is essentially an unbounded table representation, sharing the same interface as DataFrames.</p> <p>Furthermore, transformations in Structured Streaming are categorized into two types: stateless and stateful.</p> <ul> <li> <p>Stateless transformations: Operations such as <code>select()</code>, <code>filter()</code>, and <code>map()</code> do not require any information from previous or subsequent rows to process the current row; each row can be processed independently. The absence of a state makes these operations stateless.</p> </li> <li> <p>Stateful Transformations: Operations that need to maintain state. Any operation involving grouping, aggregation, or joining falls under stateful transformations. In Structured Streaming, some combinations of stateful transformations are unsupported.</p> </li> </ul> <p>Overall, transforming data in streaming can be done similarly to batch processing, with some exceptions that are outlined in the official documentation.</p> <pre><code>words = (\n    lines\n    .withColumn(\"words\", F.explode(F.split(F.col(\"lines\"), \"\\\\s\")))\n    .select(\"words\")\n)\n\ncounts = words.groupBy(\"words\").count()\n</code></pre>"},{"location":"apache-spark/structured-streaming/programming-model/#define-output-sink-and-output-mode","title":"Define output sink and output mode","text":"<p>The third step is to define where and how the transformed data will be stored using the <code>DataStreamWriter</code> which is encapsulated by <code>DataFrame.writeStream</code>. Similar to <code>DataFrameWriter</code>, it can write in a variety of formats such as files and Apache Kafka. Additionally, you can write to arbitrary locations using the foreachBatch() and foreach() API methods.</p> <p>In the vocabulary of Streaming Systems, this variety of formats is referred to as an \u201coutput sink.\u201d Essentially, the sinks (or output sinks) are responsible for persisting the computed data consistently with the desired semantics, such as exactly-once or at-least-once guarantees.</p> <p>Furthermore, the <code>outputMode</code> is also defined, dictating how data will be written to the sink. The <code>outputMode</code> can be one of the following values:</p> <ul> <li>append: Only new records are written to the sink.</li> <li>complete: The sink contains the complete output of the query for each trigger.</li> <li>update: Only the updated records are written to the sink.</li> </ul> <pre><code>query = (\n    counts\n    .writeStream\n    .format(\"console\")\n    .outputMode(\"complete\")\n)\n</code></pre>"},{"location":"apache-spark/structured-streaming/programming-model/#set-processing-details","title":"Set processing details","text":"<p>O passo final antes de iniciar a query \u00e9 especificar os detalhes de triggering e checkponting.</p> <p>Atrav\u00e9s da pol\u00edtica de triggering \u00e9 que definmos quando novos dados devem ser descobertos (coletados) e processados. At\u00e9 a vers\u00e3o 3.5 do Spark, existem quatro op\u00e7\u00f5es de triggers.</p> <ul> <li> <p>default: Streaming query executes micro-batches without explicit specification, triggering the next micro-batch when the previous one completes.</p> </li> <li> <p>once: Executes a single micro-batch and then stops.</p> </li> <li> <p>available-now. Similar to queries one-time micro-batch trigger. The difference is that it will process the data in (possibly) multiple micro-batches based on the source options.</p> </li> <li> <p>fixed-time interval: Executes micro-batches at a fixed time interval.</p> </li> <li> <p>continuous: An experimental mode (as of Spark 3.0) that processes data continuously instead of micro-batches, providing lower latency (only a subset of DataFrame operations support this mode).</p> </li> </ul> <p>And the checkpoint location is where the system stores the metadata of the query, such as the current state of the processing, the offsets, and the schema of the data. This information is crucial for fault tolerance and recovery.</p> <pre><code>query = (\n    counts\n    .writeStream\n    .format(\"console\")\n    .outputMode(\"complete\")\n    .option(\"checkpointLocation\", \"/structured-streaming-basics/data/checkpoints/\")\n    .trigger(processingTime=\"10 seconds\")\n)\n</code></pre>"},{"location":"apache-spark/structured-streaming/programming-model/#start-the-query","title":"Start the query","text":"<p>Once everything has been specified, the final step is to start the query.</p> <pre><code>query.start()\n</code></pre> <p>The returned object of type <code>StreamingQuery</code> represents an active query and can be used to manage the query, which we will cover later in this chapter.</p> <p>Note that <code>start()</code> is a nonblocking method and returns once the query starts in the background. To block the main thread until query termination, use <code>StreamingQuery.awaitTermination()</code>. If the query fails, <code>awaitTermination()</code> also fails with the same exception.</p> <p>You can also use <code>awaitTermination(timeoutMillis)</code> to wait for a specified duration and <code>stop()</code> to stop the query explicitly.</p>"},{"location":"apache-spark/structured-streaming/programming-model/#conclusion","title":"Conclusion","text":"<p>To summarize, here is the complete code.</p> <pre><code>from pyspark.sql.types import StructType, StructField, StringType\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import SparkSession\n\nschema = StructType([\n    StructField(\"lines\", StringType(), True)\n])\n\nspark = SparkSession.builder.master(\"local[*]\").appName(\"WordsCount\").getOrCreate()\n\nlines = (\n    spark\n    .readStream\n    .format(\"csv\")\n    .schema(schema)\n    .option(\"header\", \"false\")\n    .load(\"/structured-streaming-basics/data/readStream/\")\n)\n\nwords = (\n    lines\n    .withColumn(\"words\", F.explode(F.split(F.col(\"lines\"), \"\\\\s\")))\n    .select(\"words\")\n)\n\ncounts = words.groupBy(\"words\").count()\n\nquery = (\n    counts\n    .writeStream\n    .format(\"console\")\n    .option(\"checkpointLocation\", \"/structured-streaming-basics/data/checkpoints/\")\n    .outputMode(\"complete\")\n)\nquery.start()\nquery.awaitTermination()\n</code></pre>"},{"location":"apache-spark/structured-streaming/programming-model/#references","title":"References","text":"<ul> <li>Learning Spark, 2<sup>nd</sup> Edition</li> <li>Structured Streaming Programming Guide</li> </ul>"},{"location":"apache-spark/structured-streaming/sinks/","title":"Sinks","text":""},{"location":"apache-spark/structured-streaming/sinks/#introduction","title":"Introduction","text":""},{"location":"apache-spark/structured-streaming/sources/","title":"Sources","text":""},{"location":"apache-spark/structured-streaming/sources/#introduction","title":"Introduction","text":"<p>A source is the abstraction that enables the consumption of data from a streaming data producer. It acts as a data provider by presenting the data to the stream processing engine as an unbounded table. More than just reading data, the source implementation also includes the logic to manage offsets, which are used to track the position in the data stream and enable fault-tolerance and exactly-once semantics.</p> <p>Sources are defined declaratively using the <code>SparkSession.readStream()</code> method, which exposes a <code>DataStreamReader</code>. This interface allows users to specify a streaming source through the <code>format()</code> method and set source-specific options using the <code>options()</code> method. Once the source is defined, the <code>load()</code> method is called to create a streaming DataFrame that represents the data stream.</p> <p>Multiple sources</p> <p>A streaming query can define multiple input sources (streaming and batch), which can be combined using <code>DataFrame</code> operations like unions and joins.</p> <p>Lazy loading</p> <p>The loading of streaming sources is a lazy operation. Initially, what is obtained is a representation of the stream, encapsulated within a streaming DataFrame instance from which transformations can be defined to implement specific business logic.</p> <p>Data consumption and processing only begin when the stream is materialized through the initiation of a query.</p> <p>Spark Structured Streaming natively supports reading data streams from various sources. These include TCP sockets, Apache Kafka, and numerous file-based formats that the DataFrameReader also supports, such as Parquet, ORC, JSON, CSV and text files.</p> <p>Until Spark 3.5, the native sources implemented are as follows:</p> <ul> <li>File-based formats (Parquet, ORC, JSON, CSV, text files): These sources continuously monitor a specified directory, treating it as a data stream where new files are regularly added. Once detected, these files are read and processed based on their modification times. Files must be placed completely and independently in the directory (atomically).</li> <li>Kafka. Connects to Apache Kafka as a consumer to retrieve data from Kafka topics.</li> <li>Socket: Establishes a connection to a TCP server and reads UTF8 text data from it. The listening server socket is at the driver. Used only for testing since doesn't not provide end-to-end fault-tolerance guarantees.</li> <li>Rate: Generates a stream of rows at a specified rate. Each rows contains a <code>timestamp</code> and <code>value</code> column. This source is intended for testing and benchmarking.</li> <li>Rate Per Micro-Batch: This source generates data at a specified number of rows per micro-batch, with each row containing a <code>timestamp</code> and a <code>value</code>. Unlike rate data source, this data source provides a consistent set of input rows per micro-batch regardless of query execution (configuration of trigger, query being lagging, etc.). This source is intended for testing and benchmarking.</li> </ul> <p>Practical examples</p> <p>In the repository, there examples of how each source can be used within the different options available.</p>"},{"location":"apache-spark/structured-streaming/sources/#replayability","title":"Replayability","text":"<p>To guarantee fault tolerance and exactly-once semantics, one of the requirements is that the source must be replayable. This means that the source must be able to retrieve a part of the stream that had already been requested but not yet committed. In Structured Streaming, this is done by managing offsets.</p> <p>To a source to be replayable, it must be reliable. A source is considered reliable when it can produce an uncommitted offset range even after a total failure of the Structured Streaming process. Then, in this failure recovery process, offsets are restored from their last known checkpoint and requested again from the source.</p> <p>Replayability and idempotence</p> <p>The second requirement for exactly-once semantic is that the sink must be idempotent. More details about it in the Sinks section.</p> <p>Non-replayable sources</p> <p>Not all native sources are replayable. For example, the socket source is not replayable because it doesn't have a way to retrieve data that was already sent. In this case, the source is not able to provide the data that was lost in the failure recovery process.</p>"},{"location":"apache-spark/structured-streaming/sources/#offsets","title":"Offsets","text":"<p>The concept behind the source interface is that streaming data is a continuous flow of events over time that can be seen as a sequence, indexed with a monotonously incrementing counter (i.e. offsets).</p> <p>Offsets are used to request data from the external source and to indicate what data has already been consumed. Every streaming source is assumed to have offsets (similar to Kafka offsets, or Kinesis sequence numbers) to track the read position in the stream.</p> <p>Structured Streaming knows when there is data to process by asking the current offset from the external system and comparing it to the last processed offset. The data to be processed is requested by getting a batch between two offsets start and end (in the practice, it's done calling <code>getBatch</code> operation with the offset range that we want to receive)</p> <p>The source is informed that the data has been processed when Spark commit a given offset. The engine uses checkpointing and write-ahead logs to record the offset range of the data being processed in each trigger (to recover from eventual failure, those offsets are often checkpointed to external storage).</p> <p>The source contract guarantees that all data with an offset less than or equal to the committed offset has been processed and that subsequent requests will stipulate only offsets greater than that committed offset.</p> <p>It's just an abstraction</p> <p>The current explanation might give the impression that external source systems must understand or implement the concept of offsets, but this is not the case. It is a combination of the external source system's capabilities and the source implementation.</p> <p>As mentioned previously, Apache Spark natively supports various data streams. This support stems from the source abstraction, where each source is implemented in a particular way. Offsets are one of the concepts abstracted in the source abstraction.</p> <p>An example is file-based sources, where Spark treats the presence of new files as an increment in offset, even though the underlying files do not possess an inherent concept of offsets, unlike Kafka.</p>"},{"location":"apache-spark/structured-streaming/sources/#schemas","title":"Schemas","text":"<p>In Apache Spark's structured APIs, schemas are essential as they outline the data's structure, enabling optimizations from query planning to storage. Data sources like JSON or CSV files require predefined schemas for accurate parsing, while other sources might have fixed schemas focusing on metadata, leaving data interpretation to the application.</p> <p>Schema-driven design in streaming applications enhances the understanding and organization of data flows across multiple stages. Schemas in Structured Streaming can be defined programmatically using <code>StructType</code> and <code>StructField</code> to build complex structures, or they can be inferred from Scala's case classes, which simplifies the process and maintains type safety. Alternatively, schemas can be extracted from existing datasets like Parquet files, useful during prototyping but requiring maintenance to avoid complexity.</p>"},{"location":"apache-spark/structured-streaming/sources/#custom-sources","title":"Custom Sources","text":"<p>Structured Streaming enables the use of custom sources by either extending the Source interface or incorporating sources provided by third parties through custom JARs and libraries.</p> <p>An interesting additional source that Apache Spark supports (natively) is Delta Tables, a robust storage file format that enables ACID transactions for Spark and big data workloads. Delta Tables are designed to allow reliable and concurrent reads and writes on datasets, which is ideal for complex data pipelines and streaming scenarios.</p>"},{"location":"apache-spark/structured-streaming/sources/#references","title":"References","text":"<ul> <li>Stream Processing with Apache Spark</li> <li>Learning Spark, 2<sup>nd</sup> Edition</li> <li>Structured Streaming Programming Guide</li> </ul>"},{"location":"data-engineering/","title":"Data Engineering","text":""},{"location":"data-engineering/incremental-data-loading/","title":"Incremental Data Loading: Patterns and Principles","text":"<p>Incremental data loading, frequently referred to as incremental ingestion or delta loading, is the data integration approach where only new, modified, or deleted data is extracted from source systems and loaded into target systems. Unlike full load patterns, which reprocess entire datasets during every extraction cycle, incremental strategies process only the state changes\u2014the \"delta\"\u2014that have occurred since the last successful ingestion.</p> <p>This approach is the standard for scalable modern data pipelines, minimizing computational overhead, reducing network latency, and enabling fast data delivery. However, it introduces architectural complexity regarding data consistency and state tracking.</p>"},{"location":"data-engineering/incremental-data-loading/#the-incremental-loading-framework","title":"The Incremental Loading Framework","text":"<p>A robust incremental pipeline has three distinct architectural pillars: Change Detection, State Management, and Target Reconciliation.</p>"},{"location":"data-engineering/incremental-data-loading/#1-change-detection","title":"1. Change Detection","text":"<p>Change detection is the mechanism used to identify the delta\u2014the subset of data that has changed since the last successful extraction. It can be:</p> <ul> <li>Attribute-based: Use source system timestamps or monotonically increasing IDs to filter records.</li> <li>Log-based: Leveraging Change Data Capture (CDC) to read database transaction logs (WAL, binlogs) for insert, update, and delete events.</li> <li>Difference-based: Comparing hashes or full snapshots to identify drift between source and target.</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#2-state-management","title":"2. State Management","text":"<p>State management is the system's memory. It tracks the pipeline's progress to ensure data is processed exactly once (conceptually) or at least once (mechanically).</p> <ul> <li>Watermarking: Storing the maximum timestamp or ID processed in the previous run (e.g., <code>last_watermark = '2024-01-15T12:00:00'</code>).</li> <li>Checkpointing: Persisting offsets (e.g., Kafka offsets) or file listing cursors to allow the pipeline to resume from the exact point of failure or completion.</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#3-target-reconciliation","title":"3. Target Reconciliation","text":"<p>Often referred to as \"Target Handling,\" this is the strategy for merging incoming deltas into the destination storage to ensure a consistent view. The most common approaches are:</p> <ul> <li>Append-Only: Strictly adding new immutable events (e.g., logs).</li> <li>Merge/Upsert: Updating existing records based on primary keys while inserting new ones.</li> <li>Partition Overwrite: Replacing specific time-based partitions entirely to ensure atomicity.</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#strategic-evaluation-incremental-vs-full-load","title":"Strategic Evaluation: Incremental vs. Full Load","text":"<p>While incremental loading is efficient, it is not universally superior. It trades the simplicity of a full refresh for the efficiency of delta processing.</p> Feature Full Load Incremental Load Data Volume Processes 100% of data per run. Processes only changed records (\\&lt;1% typically). Complexity Low. State tracking is unnecessary. High. Requires state management and strict ordering. Resource Cost Linear growth with dataset size. Constant/Linear growth with change rate. Deletes Automatically handles deletions (missing rows are gone). Requires explicit handling (CDC or Tombstones). Consistency High. \"What you see is what you get.\" Risk of drift if pipeline logic fails. <p>Use Incremental Loading When:</p> <ul> <li>Source datasets are massive (GBs to PBs) and full scans are computationally prohibitive.</li> <li>Data availability requirements demand high-frequency updates (low latency).</li> <li>Source systems are sensitive to query load (e.g., operational OLTP databases).</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#change-detection-strategies","title":"Change Detection Strategies","text":""},{"location":"data-engineering/incremental-data-loading/#pattern-1-high-water-mark-time-based","title":"Pattern 1: High-Water Mark (Time-Based)","text":"<p>Time-based watermarking is one of the most common batch incremental loading patterns. It relies on timestamp columns in source data to identify records that have been modified since the last extraction. The pipeline queries the source for records where the modification timestamp is greater than the stored high-water mark from the previous run, which is maintained through state management.</p>"},{"location":"data-engineering/incremental-data-loading/#how-it-works","title":"How It Works","text":"<p>The pipeline maintains a watermark\u2014a timestamp representing the last successfully processed record. On each run, the pipeline queries the source system for records where the timestamp column is greater than the watermark value.</p> <pre><code>SELECT *\nFROM source_table\nWHERE updated_at &gt; '2024-01-15 10:30:00'\nORDER BY updated_at\n</code></pre> <p>After successful processing, the watermark is updated to the maximum timestamp from the current batch.</p>"},{"location":"data-engineering/incremental-data-loading/#interval-based-query-variant","title":"Interval-Based Query Variant","text":"<p>While the standard high-water mark pattern uses an open-ended query (<code>WHERE updated_at &gt; watermark</code>), some scenarios benefit from bounded interval queries that extract data within a specific time window. Interval-based queries are useful for:</p> <ul> <li>Reprocessing historical data: Re-extracting data for a specific time period without affecting the watermark</li> <li>Partitioned extraction: Processing data in fixed-size windows (e.g., daily, hourly) to manage large datasets</li> <li>Look-back windows: Including a safety buffer to handle late-arriving data while maintaining a bounded extraction window</li> <li>Parallel processing: Enabling multiple pipeline instances to process non-overlapping time intervals concurrently</li> </ul> <p>Bounded Interval Pattern:</p> <pre><code>-- Extract data for a specific time interval\nSELECT *\nFROM source_table\nWHERE updated_at &gt;= '2024-01-15 00:00:00'\n  AND updated_at &lt; '2024-01-16 00:00:00'\nORDER BY updated_at\n</code></pre> <p>Sliding Window Pattern:</p> <pre><code>-- Extract data with a look-back window for safety\nSELECT *\nFROM source_table\nWHERE updated_at &gt; '2024-01-15 10:30:00'  -- last watermark\n  AND updated_at &lt;= '2024-01-15 12:00:00'  -- current processing window end\nORDER BY updated_at\n</code></pre> <p>Implementation Considerations:</p> <p>When using interval-based queries, the watermark update strategy differs from the standard pattern. Instead of updating to the maximum timestamp in the batch, the watermark is typically advanced to the end of the processed interval:</p> <pre><code># Process data for a specific interval\nstart_time = '2024-01-15 00:00:00'\nend_time = '2024-01-15 23:59:59'\n\ndf = extract_data_between(start_time, end_time)\nprocess_and_load(df)\n\n# Update watermark to end of interval (not max timestamp in data)\nupdate_watermark(end_time)\n</code></pre> <p>This approach ensures that even if no records exist within an interval, the watermark still advances, preventing the pipeline from reprocessing empty intervals. Interval-based extraction is particularly effective when combined with partition-based target reconciliation strategies, where each interval corresponds to a target partition.</p>"},{"location":"data-engineering/incremental-data-loading/#state-management-metadata-table-approach","title":"State Management: Metadata Table Approach","text":"<p>The high-water mark is typically stored in a dedicated metadata table that tracks pipeline execution state. This table serves as the single source of truth for pipeline progress, enabling robust failure recovery and auditability. By decoupling the extraction logic from the target system's state, the metadata table allows the same pipeline to write to multiple targets (e.g., staging and production) without watermark conflicts, or to rebuild targets without losing extraction state.</p> <p>Storing watermarks in a transactional database (separate from the target) ensures transactional integrity: the watermark update can be part of the same transaction as the extraction logic, guaranteeing atomicity. If the extraction fails, the watermark is not advanced, preventing data loss.</p> <p>Metadata tables track multiple categories of information beyond just the watermark value: run identifiers for each execution, status information (success, failure, in-progress), pipeline state (watermark values, execution timestamps), and general metadata (row counts, error messages). This comprehensive tracking enables historical analysis, debugging, and compliance\u2014answering questions like \"What was the watermark on January 15<sup>th</sup>?\" or \"How many rows were processed in the last 30 days?\"</p> <p>Additionally, multiple pipeline instances can check the metadata table to determine if another instance is already running, preventing duplicate extractions and ensuring concurrent execution safety.</p>"},{"location":"data-engineering/incremental-data-loading/#alternative-target-derived-watermark","title":"Alternative: Target-Derived Watermark","text":"<p>Alternatively, the watermark can be derived from the target system by querying the maximum timestamp value present in the destination table:</p> <pre><code>SELECT MAX(updated_at) as last_watermark\nFROM target_table;\n</code></pre> <p>This approach introduces several significant risks:</p> <ul> <li>Tight Coupling: Extraction logic becomes dependent on target storage, making it difficult to support multiple targets or change target systems.</li> <li>Multiple Target Incompatibility: Cannot support independent pipeline runs when writing to multiple targets (staging, production, analytics).</li> <li>Data Loss Risk: If the target table is truncated, corrupted, or manually modified, the watermark becomes incorrect, potentially causing permanent data loss.</li> <li>No Audit Trail: Lacks historical tracking of pipeline execution, making debugging and compliance difficult.</li> <li>Concurrent Execution Issues: Multiple pipeline instances cannot safely coordinate without additional locking mechanisms.</li> </ul> <p>Despite these pitfalls, target-derived watermarks can be safely used in specific scenarios:</p> <ol> <li> <p>Single Target, Append-Only Workloads: When there is exactly one target system and the workload is strictly append-only (no updates or deletes), the target table's maximum timestamp accurately reflects the last processed record. This is common in event streaming pipelines where records are immutable.</p> </li> <li> <p>Read-Only Target Systems: If the target system is read-only or has strict access controls preventing truncation or manual modifications, the risk of corruption is minimized. Data warehouses with role-based access control (RBAC) that prevents <code>DELETE</code> or <code>TRUNCATE</code> operations fall into this category.</p> </li> <li> <p>Idempotent Merge Operations: When using idempotent merge/upsert operations (e.g., Delta Lake <code>MERGE</code>), reprocessing records is safe. Even if the watermark is temporarily incorrect due to target modifications, re-running the pipeline will correct the state without data corruption.</p> </li> <li> <p>Development and Testing: For non-production environments where data loss is acceptable, target-derived watermarks simplify setup and reduce infrastructure dependencies.</p> </li> </ol>"},{"location":"data-engineering/incremental-data-loading/#alternative-technology-specific-state-management","title":"Alternative: Technology-Specific State Management","text":"<p>Some modern data engineering tools provide built-in mechanisms for managing incremental loading state, abstracting away the manual metadata table implementation. These tools handle watermark persistence, execution tracking, and state recovery automatically.</p> <p>dbt (Data Build Tool)</p> <p>dbt's incremental materialization strategy automatically manages state through its internal metadata system. When using the <code>incremental</code> strategy with a <code>unique_key</code> and <code>incremental_strategy</code>, dbt tracks which records have been processed:</p> <pre><code>{{\n  config(\n    materialized='incremental',\n    unique_key='id',\n    incremental_strategy='merge'\n  )\n}}\n\nselect *\nfrom {{ source('raw', 'orders') }}\n{% if is_incremental() %}\n  where updated_at &gt; (select max(updated_at) from {{ this }})\n{% endif %}\n</code></pre> <p>dbt stores execution metadata in its internal <code>dbt_meta</code> schema, tracking run history, model execution times, and incremental state. The <code>is_incremental()</code> macro automatically detects whether this is a full refresh or incremental run, and dbt manages the watermark comparison internally.</p> <p>Apache Spark (Databricks)</p> <p>Databricks Auto Loader provides automatic state management for incremental data loading by tracking processed files through checkpoint locations. While Auto Loader uses Structured Streaming (<code>readStream</code>) under the hood, it can be configured to run as batch jobs using <code>Trigger.AvailableNow</code>, which processes all available files and then stops, eliminating the need for manual metadata table management:</p> <pre><code>from pyspark.sql.streaming import Trigger\n\n# Auto Loader with Trigger.AvailableNow for batch-like behavior\ndf = spark.readStream \\\n    .format(\"cloudFiles\") \\\n    .option(\"cloudFiles.format\", \"parquet\") \\\n    .option(\"cloudFiles.schemaLocation\", checkpoint_path) \\\n    .load(source_path)\n\n# Trigger.AvailableNow processes all files available at start time, then stops\nquery = df.writeStream \\\n    .format(\"delta\") \\\n    .option(\"checkpointLocation\", checkpoint_path) \\\n    .trigger(Trigger.AvailableNow()) \\\n    .start(target_table)\n\nquery.awaitTermination()\n</code></pre> <p>Auto Loader automatically tracks processed files in the checkpoint location using RocksDB, storing file paths, modification times, and processing status. Subsequent runs only process new files that arrived since the last execution, enabling automatic deduplication and incremental processing without manual watermark tracking.</p> <p>For time-based incremental loading with Delta Lake tables, you can leverage Delta's time travel features to derive watermarks from the target table:</p> <pre><code># Time-based incremental load using Delta time travel for watermark\nfrom delta.tables import DeltaTable\n\n# Get last processed timestamp from target Delta table\ndelta_table = DeltaTable.forPath(spark, target_table)\nlast_watermark = spark.sql(f\"\"\"\n    SELECT MAX(updated_at) as max_timestamp \n    FROM {target_table}\n\"\"\").collect()[0]['max_timestamp'] or \"1970-01-01\"\n\n# Extract new records from source\ndf = spark.read.format(\"parquet\").load(source_path)\ndf_filtered = df.filter(df.updated_at &gt; last_watermark)\n\n# Merge into target Delta table (idempotent operation)\ndelta_table.alias(\"target\").merge(\n    df_filtered.alias(\"source\"),\n    \"target.id = source.id\"\n).whenMatchedUpdateAll() \\\n .whenNotMatchedInsertAll() \\\n .execute()\n</code></pre> <p>The checkpoint location stores file-level metadata, enabling automatic file deduplication across batch runs, while Delta Lake's ACID transactions and time travel capabilities provide additional state management and recovery mechanisms.</p> <p>Apache Airflow</p> <p>Airflow provides strong support for bounded interval-based incremental loading through its data interval concept. Each DAG run is associated with a logical date and data interval (<code>data_interval_start</code> and <code>data_interval_end</code>), which naturally maps to time-bounded extraction windows. This makes Airflow particularly well-suited for interval-based incremental loading patterns:</p> <pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.models import Variable\nfrom datetime import datetime, timedelta\n\ndef bounded_interval_load(**context):\n    # Airflow provides data_interval_start and data_interval_end in context\n    interval_start = context['data_interval_start']\n    interval_end = context['data_interval_end']\n\n    # Extract records for the specific interval (bounded query)\n    query = f\"\"\"\n        SELECT *\n        FROM source_table\n        WHERE updated_at &gt;= '{interval_start}'\n          AND updated_at &lt; '{interval_end}'\n        ORDER BY updated_at\n    \"\"\"\n\n    new_records = execute_query(query)\n    process_and_load(new_records)\n\n# Daily DAG with bounded intervals\ndag = DAG(\n    'incremental_orders',\n    schedule='@daily',  # Each run processes one day's data\n    start_date=datetime(2024, 1, 1),\n    catchup=False\n)\n\ntask = PythonOperator(\n    task_id='load_incremental',\n    python_callable=bounded_interval_load,\n    dag=dag\n)\n</code></pre> <p>Airflow's data intervals automatically handle bounded extraction windows: each DAG run processes data for its assigned interval (e.g., <code>2024-01-15 00:00:00</code> to <code>2024-01-16 00:00:00</code> for a daily schedule). This eliminates the need for manual watermark tracking when using interval-based patterns, as Airflow ensures each interval is processed exactly once.</p> <p>For standard high-water mark patterns, Airflow's XCom (cross-communication) and Task Instance context provide mechanisms for state persistence:</p> <pre><code>def incremental_load(**context):\n    # Retrieve last watermark from Airflow Variables or XCom\n    last_watermark = Variable.get(\"orders_last_watermark\", default_var=\"2024-01-01\")\n\n    # Extract new records (open-ended query)\n    new_records = extract_since_watermark(last_watermark)\n\n    # Process and load\n    process_and_load(new_records)\n\n    # Update watermark in Airflow Variables\n    new_watermark = get_max_timestamp(new_records)\n    Variable.set(\"orders_last_watermark\", new_watermark)\n</code></pre> <p>Airflow's bounded interval capabilities make it ideal for partitioned extraction strategies where each run processes a fixed time window, while Variables, XCom, and the metadata database enable custom watermark-based incremental loading with built-in execution tracking and retry mechanisms.</p> <p>Trade-offs of Technology-Specific Approaches:</p> <ul> <li>Simplified Implementation: Reduces boilerplate code and manual state management overhead.</li> <li>Tool Lock-in: State management becomes dependent on the specific tool's implementation and migration paths.</li> <li>Limited Customization: May not support complex state tracking requirements or multi-target scenarios.</li> <li>Vendor-Specific Behavior: Understanding tool-specific state management nuances is required for production reliability.</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#advantages","title":"Advantages","text":"<ul> <li>Simple to implement and understand</li> <li>Works well with append-only workloads</li> <li>Low overhead on source systems (indexed timestamp columns)</li> <li>Natural ordering ensures consistent processing</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#limitations","title":"Limitations","text":"<ul> <li>Requires reliable timestamp columns in source data</li> <li>May miss records if timestamps are not updated on every change</li> <li>Vulnerable to clock skew issues in distributed systems</li> <li>Cannot detect hard deletes (records removed from source)</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#best-practices","title":"Best Practices","text":"<ul> <li>Use transaction (processing) timestamps or system-generated timestamps rather than business-related timestamps.</li> <li>Indexed timestamp columns significantly improve query performance.</li> <li>Store watermarks in a durable, transactional system (database, metadata store).</li> <li>Consider using multiple timestamp columns (created_at, updated_at) for comprehensive change detection.</li> <li>Implement tolerance windows to handle late-arriving data.</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#project","title":"Project","text":"<p>I have developed a project implementing the high-water mark pattern. [Link placeholder]</p>"},{"location":"data-engineering/incremental-data-loading/#pattern-2-sequence-based-monotonic-ids","title":"Pattern 2: Sequence-Based (Monotonic IDs)","text":"<p>Sequence-based incremental loading uses monotonically increasing sequence numbers or auto-increment IDs to identify new records.</p>"},{"location":"data-engineering/incremental-data-loading/#how-it-works_1","title":"How It Works","text":"<p>The pipeline tracks the maximum sequence number processed in the previous run. On each execution, it extracts records with sequence numbers greater than the last processed value.</p> <pre><code>SELECT *\nFROM source_table\nWHERE id &gt; 1234567\nORDER BY id\n</code></pre>"},{"location":"data-engineering/incremental-data-loading/#advantages_1","title":"Advantages","text":"<ul> <li>Simple and efficient</li> <li>Guaranteed ordering</li> <li>Works well for append-only workloads</li> <li>No timestamp dependency</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#limitations_1","title":"Limitations","text":"<ul> <li>Only detects new records, not updates or deletes</li> <li>Requires sequential, non-reusable IDs</li> <li>Gaps in sequences can cause issues</li> <li>Not suitable for update-heavy workloads</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#state-management","title":"State Management","text":"<p>The state management approach for sequence-based watermarking follows the same principles as Pattern 1 (High-Water Mark). The maximum <code>id</code> processed is stored in a metadata table that tracks the same categories of information: run identifiers, status information, pipeline state (sequence numbers instead of timestamps), and general metadata. This provides identical benefits: decoupling from target storage, transactional integrity, comprehensive pipeline run tracking, audit trail, and concurrent execution safety. The same considerations and pitfalls regarding target-derived watermarks apply\u2014refer to Pattern 1's \"State Management: Metadata Table Approach\" and \"Alternative: Target-Derived Watermark\" sections for detailed explanation.</p>"},{"location":"data-engineering/incremental-data-loading/#project_1","title":"Project","text":"<p>I have developed a project implementing the sequence-based pattern. [Link placeholder]</p>"},{"location":"data-engineering/incremental-data-loading/#pattern-3-change-data-capture-cdc","title":"Pattern 3: Change Data Capture (CDC)","text":"<p>Change Data Capture is a pattern that captures changes at the source system level, typically by reading database transaction logs or using database-specific CDC mechanisms.</p>"},{"location":"data-engineering/incremental-data-loading/#how-it-works_2","title":"How It Works","text":"<p>CDC systems monitor database transaction logs (redo logs, write-ahead logs) to identify INSERT, UPDATE, and DELETE operations as they occur. These changes are captured and streamed to downstream systems in near real-time.</p> <p>The CDC process typically follows these steps:</p> <ol> <li> <p>Log Reading: The CDC connector reads the database's transaction log (e.g., PostgreSQL WAL, MySQL binlog, Oracle redo logs) at a specific position or LSN (Log Sequence Number).</p> </li> <li> <p>Change Parsing: Each log entry is parsed to extract the operation type (INSERT, UPDATE, DELETE), affected table, row data (before/after values for updates), and transaction metadata.</p> </li> <li> <p>Change Event Generation: The parsed changes are transformed into standardized change events, typically in formats like Debezium's change event format or AWS DMS's JSON format.</p> </li> <li> <p>Streaming: Change events are published to a message queue (Kafka, Kinesis) or directly to downstream consumers.</p> </li> <li> <p>Offset Tracking: The CDC connector maintains its position in the transaction log, allowing it to resume from the exact point of failure.</p> </li> </ol> <p>Example: PostgreSQL Logical Replication with Debezium</p> <pre><code>{\n  \"before\": {\n    \"id\": 12345,\n    \"customer_name\": \"Acme Corp\",\n    \"order_total\": 1500.00,\n    \"updated_at\": \"2024-01-15T10:00:00Z\"\n  },\n  \"after\": {\n    \"id\": 12345,\n    \"customer_name\": \"Acme Corporation\",\n    \"order_total\": 1500.00,\n    \"updated_at\": \"2024-01-15T12:30:00Z\"\n  },\n  \"source\": {\n    \"version\": \"2.5.0\",\n    \"connector\": \"postgresql\",\n    \"name\": \"orders-connector\",\n    \"ts_ms\": 1705321800000,\n    \"snapshot\": false,\n    \"db\": \"production\",\n    \"schema\": \"public\",\n    \"table\": \"orders\",\n    \"txId\": 54321,\n    \"lsn\": 123456789,\n    \"xmin\": null\n  },\n  \"op\": \"u\",\n  \"ts_ms\": 1705321800123\n}\n</code></pre> <p>In this example: - <code>op: \"u\"</code> indicates an UPDATE operation - <code>before</code> contains the row state before the change - <code>after</code> contains the row state after the change - <code>source.lsn</code> (Log Sequence Number) tracks the position in PostgreSQL's WAL - <code>source.ts_ms</code> is the timestamp when the change was committed</p> <p>Pipeline Processing:</p> <pre><code># Simplified CDC event processing\nfor change_event in cdc_stream:\n    if change_event['op'] == 'c':  # Create/Insert\n        target_table.insert(change_event['after'])\n    elif change_event['op'] == 'u':  # Update\n        target_table.upsert(\n            key=change_event['after']['id'],\n            data=change_event['after']\n        )\n    elif change_event['op'] == 'd':  # Delete\n        target_table.delete(change_event['before']['id'])\n\n    # Update offset to commit progress\n    offset_tracker.update(change_event['source']['lsn'])\n</code></pre> <p>Common CDC implementations include:</p> <ul> <li>Database-native CDC: PostgreSQL logical replication, MySQL binlog, Oracle GoldenGate</li> <li>Log-based CDC: Debezium, AWS DMS, Fivetran</li> <li>Trigger-based CDC: Database triggers that write changes to change tables</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#advantages_2","title":"Advantages","text":"<ul> <li>Captures all changes, including hard deletes</li> <li>Near real-time change detection</li> <li>Low impact on source systems (reads logs, not tables)</li> <li>Preserves transaction boundaries and ordering</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#limitations_2","title":"Limitations","text":"<ul> <li>Requires access to database logs (may have security/compliance restrictions)</li> <li>More complex to implement and operate</li> <li>May require additional infrastructure (CDC servers, message queues)</li> <li>Database-specific implementations reduce portability</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#project_2","title":"Project","text":"<p>I have developed a project implementing the CDC pattern. [Link placeholder]</p>"},{"location":"data-engineering/incremental-data-loading/#pattern-4-snapshot-differencing","title":"Pattern 4: Snapshot Differencing","text":"<p>Snapshot differencing compares full snapshots of data between consecutive pipeline runs to identify differences. This approach computes the set difference between snapshots to identify inserted, updated, and deleted records. It is typically used when source systems lack reliable timestamps, sequence columns, or transaction logs.</p>"},{"location":"data-engineering/incremental-data-loading/#how-it-works_3","title":"How It Works","text":"<ol> <li>Snapshot Extraction: Extract a complete snapshot of source data at time T1 and store it (snapshot S1).</li> <li>Subsequent Snapshot: Extract another complete snapshot at time T2 (snapshot S2).</li> <li>Difference Computation: Compare S1 and S2 to identify:</li> <li>Inserts: Records present in S2 but not in S1</li> <li>Updates: Records present in both but with different attribute values</li> <li>Deletes: Records present in S1 but not in S2</li> <li>Delta Processing: Process only the identified changes (delta).</li> <li>Snapshot Rotation: Replace S1 with S2 for the next comparison cycle.</li> </ol>"},{"location":"data-engineering/incremental-data-loading/#advantages_3","title":"Advantages","text":"<ul> <li>Simple conceptual model</li> <li>Can detect all types of changes</li> <li>No dependency on source system features</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#limitations_3","title":"Limitations","text":"<ul> <li>Requires storing full snapshots (storage overhead)</li> <li>Comparison step can be expensive</li> <li>Not suitable for very large datasets</li> <li>May require significant memory for comparison</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#implementation-variants","title":"Implementation Variants","text":"<p>Since comparing full snapshots is computationally expensive, several optimization strategies exist. The most common approaches include hash-based comparison, key-based differencing, and incremental snapshot storage.</p>"},{"location":"data-engineering/incremental-data-loading/#hash-based-change-detection","title":"Hash-Based Change Detection","text":"<p>Hash-based change detection computes a cryptographic hash value (MD5, SHA-256) for each record based on its attributes. By comparing hash values between source and target, the system can identify changed records without storing full snapshots.</p> <p>How It Works:</p> <ol> <li> <p>Hash Computation: For each record, compute a hash value using all relevant columns (excluding metadata columns like <code>updated_at</code> if they exist).</p> </li> <li> <p>Hash Storage: Store the hash value alongside the record in the target system, typically in a dedicated <code>record_hash</code> column.</p> </li> <li> <p>Comparison: On each pipeline run:</p> </li> <li>Compute hashes for all source records</li> <li>Compare source hashes with stored target hashes</li> <li>Identify records where:<ul> <li>Hash exists in source but not in target \u2192 INSERT</li> <li>Hash exists in both but values differ \u2192 UPDATE</li> <li>Hash exists in target but not in source \u2192 DELETE</li> </ul> </li> </ol> <p>Example Implementation:</p> <pre><code>import hashlib\nimport json\n\ndef compute_record_hash(record, columns_to_hash):\n    \"\"\"Compute SHA-256 hash for a record based on specified columns.\"\"\"\n    # Sort columns for consistent hashing\n    record_data = {col: record[col] for col in sorted(columns_to_hash)}\n    record_json = json.dumps(record_data, sort_keys=True, default=str)\n    return hashlib.sha256(record_json.encode()).hexdigest()\n\n# Pipeline execution\nsource_records = extract_source_data()\ntarget_records = load_target_data()\n\n# Compute hashes for source\nsource_hashes = {\n    compute_record_hash(record, ['id', 'name', 'email', 'status']): record\n    for record in source_records\n}\n\n# Load existing hashes from target\ntarget_hashes = {\n    record['record_hash']: record\n    for record in target_records\n}\n\n# Identify changes\ninserts = [\n    source_hashes[hash_val]\n    for hash_val in source_hashes\n    if hash_val not in target_hashes\n]\n\nupdates = [\n    source_hashes[hash_val]\n    for hash_val in source_hashes\n    if hash_val in target_hashes\n    and source_hashes[hash_val] != target_hashes[hash_val]\n]\n\ndeletes = [\n    target_hashes[hash_val]\n    for hash_val in target_hashes\n    if hash_val not in source_hashes\n]\n\n# Process delta\nprocess_inserts(inserts)\nprocess_updates(updates)\nprocess_deletes(deletes)\n</code></pre> <p>SQL-Based Hash Comparison:</p> <pre><code>-- Step 1: Create staging table with computed hashes\nCREATE TEMP TABLE source_with_hash AS\nSELECT \n    *,\n    SHA256(CONCAT(id, name, email, status)) as record_hash\nFROM source_table;\n\n-- Step 2: Identify inserts (new hashes)\nSELECT s.*\nFROM source_with_hash s\nLEFT JOIN target_table t ON s.record_hash = t.record_hash\nWHERE t.record_hash IS NULL;\n\n-- Step 3: Identify updates (hash exists but data changed)\nSELECT s.*\nFROM source_with_hash s\nINNER JOIN target_table t ON s.id = t.id\nWHERE s.record_hash != t.record_hash;\n\n-- Step 4: Identify deletes (hash missing in source)\nSELECT t.*\nFROM target_table t\nLEFT JOIN source_with_hash s ON t.record_hash = s.record_hash\nWHERE s.record_hash IS NULL;\n</code></pre> <p>Optimization Strategies:</p> <ul> <li>Incremental Hash Comparison: Only compare hashes for records where the primary key exists in both source and target, reducing comparison scope.</li> <li>Partitioned Hashing: Compute hashes per partition to enable parallel processing and reduce memory requirements.</li> <li>Hash Indexing: Create indexes on <code>record_hash</code> columns to accelerate lookups during comparison.</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#advantages_4","title":"Advantages","text":"<ul> <li>Detects changes regardless of change type (insert, update, delete)</li> <li>Works without timestamp or sequence columns</li> <li>Can detect changes in any column</li> <li>More storage-efficient than full snapshot storage (stores hashes, not full records)</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#limitations_4","title":"Limitations","text":"<ul> <li>Computationally expensive (requires full table scan and hash computation)</li> <li>May not scale well for very large tables (millions+ records)</li> <li>Requires storing hash values in target system (additional column overhead)</li> <li>Hash collisions are theoretically possible (SHA-256 collision probability is negligible for practical purposes)</li> <li>Cannot identify which specific column changed (only that something changed)</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#project_3","title":"Project","text":"<p>I have developed a project implementing the snapshot differencing pattern. [Link placeholder]</p>"},{"location":"data-engineering/incremental-data-loading/#target-reconciliation-strategies","title":"Target Reconciliation Strategies","text":""},{"location":"data-engineering/incremental-data-loading/#append-only","title":"Append-Only","text":"<p>New records are appended without modifying existing ones. Ideal for immutable event data (logs, transactions, sensor readings). This strategy provides the highest write throughput and simplest consistency model.</p>"},{"location":"data-engineering/incremental-data-loading/#mergeupsert","title":"Merge/Upsert","text":"<p>INSERT new records and UPDATE existing ones based on a business key. Common for slowly changing dimensions (SCD) and transactional data. This approach maintains referential integrity and supports point-in-time queries.</p>"},{"location":"data-engineering/incremental-data-loading/#partition-overwrite","title":"Partition Overwrite","text":"<p>For specific partitions or date ranges, delete existing data and replace with fresh data. Useful when CDC is unavailable but partition-level refresh is acceptable. This approach ensures atomicity at the partition granularity while avoiding full table scans.</p>"},{"location":"data-engineering/incremental-data-loading/#engineering-challenges-and-resilience-patterns","title":"Engineering Challenges and Resilience Patterns","text":"<p>Incremental pipelines are prone to specific failure modes that can compromise data integrity. Robust engineering requires addressing these proactively to guarantee idempotency, prevent dataset corruption (e.g., duplicate records), and ensure fault tolerance and resilience. Pipelines will fail, and when retried, they may reprocess data. If the pipeline is not idempotent, this leads to duplicate records or incorrect aggregations.</p>"},{"location":"data-engineering/incremental-data-loading/#1-idempotency-and-duplicate-handling","title":"1. Idempotency and Duplicate Handling","text":"<p>The Problem: Pipeline failures and retries can cause duplicate processing of the same data, leading to duplicate records in the target system or incorrect aggregations.</p> <p>The Solution:</p> <ul> <li>Merge/Upsert: Never use blind <code>INSERT</code>. Use <code>MERGE INTO</code> (SQL) or Delta/Iceberg <code>merge</code> operations to update existing keys and insert new ones.</li> <li>Deduplication: Within a batch, deduplicate by primary key, keeping the record with the latest timestamp.</li> <li>Transactional Writes: Use ACID-compliant table formats (Delta Lake, Apache Iceberg) to ensure that a failed batch does not leave partial data.</li> </ul> <pre><code># Spark/Delta Lake Example of Idempotency\ndeltaTable.alias(\"t\").merge(\n    source_df.alias(\"s\"),\n    \"t.id = s.id\"\n).whenMatchedUpdateAll() \\\n .whenNotMatchedInsertAll() \\\n .execute()\n</code></pre>"},{"location":"data-engineering/incremental-data-loading/#2-the-hard-delete-problem","title":"2. The \"Hard Delete\" Problem","text":"<p>The Problem: Standard SQL queries (<code>WHERE updated_at &gt; X</code>) cannot return records that no longer exist. A hard delete in the source leaves a \"ghost\" record in the target.</p> <p>The Solution:</p> <ul> <li>Soft Deletes (Preferred): Change source behavior to set a flag (<code>is_deleted=true</code>) rather than physically removing rows.</li> <li>Tombstones (CDC): CDC connectors generate a \"delete\" event. The pipeline must interpret this event to either delete the target row or mark it as valid-to-current-time in temporal tables.</li> <li>Periodic Reconciliation: Run a low-frequency (e.g., weekly) process that performs a full outer join (key-only) to identify and purge IDs present in the target but missing in the source.</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#3-late-arriving-data-and-out-of-order-events","title":"3. Late Arriving Data and Out-of-Order Events","text":"<p>The Problem: In distributed systems, data ingestion time often lags behind event generation time. If a watermark is advanced based on \"processing time,\" a late-arriving record with an older \"event time\" may be permanently skipped.</p> <p>The Solution:</p> <ul> <li>Look-back Windows: When querying the source, include a buffer (e.g., <code>WHERE updated_at &gt; last_watermark - INTERVAL '1 hour'</code>).</li> <li>Dual-Timestamp Architecture: Track both <code>event_time</code> (business logic) and <code>processing_time</code> (watermarking). Use <code>processing_time</code> for extraction but <code>event_time</code> for ordering.</li> <li>Watermark Delay: Intentionally lag the watermark to allow for consistency windows (e.g., Spark Structured Streaming watermarks).</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#4-schema-evolution","title":"4. Schema Evolution","text":"<p>The Problem: Source schemas change (column additions, type changes). A rigid pipeline will fail or drop data when the schema drifts.</p> <p>The Solution:</p> <ul> <li>Schema-on-Read: Use file formats like Parquet or Avro that carry schema metadata, enabling schema evolution without pipeline modifications.</li> <li>Schema Registry: Implement a central registry (e.g., Confluent Schema Registry) to validate compatibility (backward/forward) before ingestion.</li> <li>Schema Evolution Features: Utilize features in Delta Lake or Snowflake that allow <code>autoMerge</code> or schema inference during write operations.</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#5-re-ingestion-and-backfilling","title":"5. Re-ingestion and Backfilling","text":"<p>The Problem: Business logic changes or data quality bugs require reprocessing historical data. Incremental pipelines are typically designed to only move forward.</p> <p>The Solution:</p> <ul> <li>Parameterization: Design the pipeline to accept explicit <code>start_date</code> and <code>end_date</code> parameters, allowing manual override of the stored watermark.</li> <li>Idempotent Replay: Ensure that re-running a historical batch overwrites the bad data without creating duplicates (see Idempotency section).</li> <li>Time Travel: Leverage modern table formats (Delta Lake, Iceberg) to rollback the target table to a previous version before re-running the load.</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#implementation-checklist","title":"Implementation Checklist","text":"<p>To certify an incremental load pipeline for production, verify the following:</p> <ul> <li>State Persistence: Is the watermark/state stored in a durable, ACID-compliant backend (not a temporary file)?</li> <li>Idempotency: Can I run the same batch 5 times and get the same result as running it once?</li> <li>Delete Strategy: How does the system react when a row is deleted in the source?</li> <li>Schema Drift: What happens if a column is added to the source tomorrow? (Fail? Ignore? Evolve?)</li> <li>Observability: Are metrics defined for \"rows extracted\" versus \"rows loaded\"? Is there an alert for \"zero rows processed\" (stale pipeline)?</li> <li>Re-ingestion: Is there a runbook for re-processing the last 30 days of data?</li> </ul>"},{"location":"data-engineering/incremental-data-loading/#conclusion","title":"Conclusion","text":"<p>Incremental loading is an exercise in managing state and failure. By standardizing on a robust Change Detection strategy, maintaining strict State Management, and implementing idempotent Target Reconciliation, engineers can build pipelines that are not only efficient but resilient to the inherent complexity and failure modes of distributed data systems.</p>"},{"location":"machine-learning-engineering/","title":"Machine Learning Engineering","text":""},{"location":"machine-learning-engineering/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Projetar, construir e manter solu\u00e7\u00f5es de machine learning n\u00e3o s\u00e3o tarefas f\u00e1ceis. Muito pelo contr\u00e1rio! S\u00e3o complexas e exigem melhorias cont\u00ednuas.</p> <p>De acordo com o relat\u00f3rio \"2020 State of Enterprise Machine Learning\" da Algorithmia.</p> <ul> <li>A maior parte das empresas ainda n\u00e3o descobriram como atingir seus objetivos de ML/IA pois a lacuna entre a constru\u00e7\u00e3o do modelo de ML e o deploy \u00e9 desafiadora.</li> <li>Apenas 22% das companhias que usam aprendizado de m\u00e1quina implantaram com sucesso um modelo de ML em produ\u00e7\u00e3o.</li> </ul> <p>E isso ocorre pois apenas uma pequena fra\u00e7\u00e3o dos sistemas de ML do mundo real s\u00e3o compostos por c\u00f3digo de ML. Um sistema em produ\u00e7\u00e3o \u00e9 composto de diversos componentes, como, por exemplo: interfaces para que usu\u00e1rios e desenvolvedores interajam com o sistema, infraestrutura para executar a aplica\u00e7\u00e3o, engenharia e governan\u00e7a de dados para o gerenciamento e confiabilidade dos dados, entre outros.</p> <p></p> <p> Fonte: Sculley, David, et al. \"Hidden technical debt in machine learning systems.\" Advances in neural information processing systems 28 (2015): 2503-2511. </p> <p>Al\u00e9m disso, considerando a escala de muitos sistemas de ML que \\(-\\) consumem grandes quantidades de dados, exigem um grande recurso computacional e afeta milhares de vidas \\(-\\) a simples necessidade de coloc\u00e1-lo em produ\u00e7\u00e3o j\u00e1 \u00e9 um grande desafio de engenharia e social. Como diz nossa querida Chip Huyen:</p> <p>Cita\u00e7\u00e3o</p> <p>\"Quando este desafio n\u00e3o \u00e9 bem compreendido, o sistema de ML pode causar grandes preju\u00edzos tanto \u00e0 companhia quanto a vida das pessoas.\" \\(-\\) Chip Huyen</p> <p>Portanto, precisamos de um conjunto de pr\u00e1ticas e processos eficazes para projetar, construir, implantar e manter modelos de ML em produ\u00e7\u00e3o de forma escal\u00e1vel e confi\u00e1vel. Andriy Burkov define machine learning engineering como a \u00e1rea respons\u00e1vel pela produtiza\u00e7\u00e3o, operacionaliza\u00e7\u00e3o e manuten\u00e7\u00e3o de sistemas de ML, sendo a defini\u00e7\u00e3o:</p> <p>Defini\u00e7\u00e3o de Machine Learning Engineering</p> <p>\"Machine learning engineering \u00e9 o uso de princ\u00edpios cient\u00edficos, ferramentas e t\u00e9cnicas de aprendizado de m\u00e1quina e engenharia de software tradicional para projetar e construir sistemas de computa\u00e7\u00e3o complexos. O MLE abrange todas as etapas, desde a coleta de dados, at\u00e9 a constru\u00e7\u00e3o do modelo, a fim de disponibilizar o modelo para uso pelo produto ou  consumidores.\" \\(\u2014\\) Andriy Burkov</p> <p>Por\u00e9m, h\u00e1 pessoas que dizem que MLOps \u00e9 a \u00e1rea respons\u00e1vel por lidar com modelos em produ\u00e7\u00e3o, sendo MLOps definido pela MLOps SIG como:</p> <p>Defini\u00e7\u00e3o de MLOps</p> <p>\"A extens\u00e3o da metodologia DevOps para incluir ativos de aprendizado de m\u00e1quina e ci\u00eancia de dados como cidad\u00e3os de primeira classe dentro da ecologia DevOps.\"</p> <p>Contudo, independente do termo utilizado (MLOps ou MLE), o que importa \u00e9 o objetivo da \u00e1rea de fornecer um processo para o projeto e desenvolvimento de sistemas baseados em machine learning que sejam reprodut\u00edveis, escal\u00e1veis e robustos.</p> <p>Aqui, MLOps estar\u00e1 restrito a tarefas relacionadas a operacionaliza\u00e7\u00e3o dos modelos de ML. Demais tarefas correspondentes a outras etapas de um projeto de ML ter\u00e3o pr\u00e1ticas classificadas como pertencentes a \u00e1rea machine learning engineering.</p> <p>Sistema x Aplica\u00e7\u00e3o</p> <p>Embora eu j\u00e1 tenha escrito \"sistema\", \"sistema\" e \"sistema\", n\u00e3o deixei claro sobre o que eu estou falando. O que \u00e9 um sistema de ML? \u00c9 a aplica\u00e7\u00e3o? Ou a solu\u00e7\u00e3o de ML que a aplica\u00e7\u00e3o usa? E essa solu\u00e7\u00e3o de ML... Usa alguma plataforma de ML para auxiliar na produ\u00e7\u00e3o de modelos? Se sim, a plataforma \u00e9 o sistema de ML?</p> <p>No caso, h\u00e1 pessoas que consideram aplica\u00e7\u00f5es baseadas em ML (i.e. que usam recursos de ML em algum momento, como uma aplica\u00e7\u00e3o que usa algoritmos de recomenda\u00e7\u00e3o) e sistemas de ML como a mesma coisa. Por\u00e9m, \"sistemas de ML\" n\u00e3o me parece transmitir bem essa ideia.</p> <p>Ent\u00e3o, vamos considerar aqui que um sistema de ML \u00e9 qualquer aplica\u00e7\u00e3o ou solu\u00e7\u00e3o que possui alguma depend\u00eancia com algoritmos de ML e, portanto, necessitam de pr\u00e1ticas espec\u00edficas para ML. Ainda, tamb\u00e9m vamos dizer que uma plataforma de ML \u00e9 uma aplica\u00e7\u00e3o que fornece um conjunto de recursos a execu\u00e7\u00e3o de pr\u00e1ticas MLOps.</p>"},{"location":"machine-learning-engineering/#machine-learning-na-industria","title":"Machine Learning na Ind\u00fastria","text":"<p>Muitas pessoas possuem o senso comum de que:</p> <ul> <li>O desenvolvimento de algoritmos e sistemas de ML \u00e9 o mesmo tanto na ind\u00fastria quanto na academia</li> <li>Desenvolver aplica\u00e7\u00f5es baseadas em ML \u00e9 o mesmo que desenvolver softwares tradicionais.</li> </ul> <p>Essa perspectiva n\u00e3o apenas est\u00e1 errada, como tamb\u00e9m \u00e9 perigosa. Sistemas de ML possuem diversos desafios pr\u00f3prios e, como j\u00e1 dito anteriormente, n\u00e3o compreender tais desafios pode levar a consequ\u00eancias ruins.</p>"},{"location":"machine-learning-engineering/#ml-na-pesquisa-x-ml-na-industria-mercado","title":"ML na Pesquisa x ML na Ind\u00fastria (Mercado)","text":"<p>O desenvolvimento de modelos de ML na academia possui um prop\u00f3sito diferente quando comparado com a ind\u00fastria.</p> <p>Na academia, (geralmente) estamos preocupados em alcan\u00e7ar o estado-da-arte (SOTA, do ingl\u00eas state-of-the-art) em um determinado problema. Por conta disso, \u00e9 muito comum que os modelos resultantes sejam custosos demais para serem utilizados na ind\u00fastria.</p> <p>Modelos com bilh\u00f5es de par\u00e2metros, por exemplo, s\u00e3o extremamente custosos para treinar e operacionalizar. Dependendo de onde ser\u00e1 feita a sua implanta\u00e7\u00e3o (e.g. dispositivo m\u00f3vel), o uso de algo t\u00e3o complexo \u00e9 invi\u00e1vel.</p> <p>Nota</p> <p>Eventualmente os \"big models\" v\u00e3o se tornar menores e mais r\u00e1pidos. Por\u00e9m, a diferen\u00e7a de prioridades entre a academia e ind\u00fastria raramente ir\u00e1 permitir que os m\u00e9todos SOTA sejam utilizados em produ\u00e7\u00e3o.</p> <p>Ainda, em projetos de pesquisa \u00e9 muito comum que os dados sejam algum \"benchmarking\". Logo, s\u00e3o dados geralmente limpos e que permitem o foco total no desenvolvimento de modelos de aprendizado. Por outro lado, sistemas em produ\u00e7\u00e3o precisam lidar com falta de dados, dados desorganizados, ru\u00eddosos e que mudam constantemente.</p> <p>De fato, tanto os objetivos quanto os desafios s\u00e3o consideralvemente diferentes em cada contexto. Contudo, ainda assim \u00e9 muito comum os profissionais de dados focarem unicamente no desenvolvimento do modelo e encarar com menor import\u00e2ncia as demais tarefas. Esta atitude \u00e9 um exemplo de quando o desafio de colocar sistemas de ML em produ\u00e7\u00e3o n\u00e3o \u00e9 bem compreendido.</p>"},{"location":"machine-learning-engineering/#softwares-tradicionais-x-sistemas-de-ml","title":"Softwares Tradicionais x Sistemas de ML","text":"<p>Diferente de softwares tradicionais, sistemas de ML n\u00e3o compreendem apenas c\u00f3digo, mas tamb\u00e9m dados e modelos. A adi\u00e7\u00e3o de mais dois artefatos torna o desenvolvimento de aplica\u00e7\u00f5es baseadas em ML significativamente mais complexo, pois al\u00e9m de testar e versionar c\u00f3digos, temos que testar e versionar dados e modelos.</p> <p>Consequentemente, desafios \u00fanicos ao projeto de sistemas de ML surgem, desde etapas como desenvolvimento, at\u00e9 o teste, integra\u00e7\u00e3o, compila\u00e7\u00e3o e monitoramento do sistema.</p>"},{"location":"machine-learning-engineering/#etapas-de-um-projeto-de-ml","title":"Etapas de um Projeto de ML","text":"<p>O projeto de um sistema de ML \u00e9 processo formado por v\u00e1rias etapas que partem desde a concep\u00e7\u00e3o at\u00e9 a manuten\u00e7\u00e3o da solu\u00e7\u00e3o encontrada. Este processo \\(-\\) tamb\u00e9m chamado de ciclo de vida \\(-\\) \u00e9 composto por quatro etapas principais (que por sua vez, podem ser divididas em mais etapas). S\u00e3o elas:</p> <ul> <li>Escopo</li> <li>Prepara\u00e7\u00e3o dos Dados</li> <li>Modelagem</li> <li>Implanta\u00e7\u00e3o (do ingl\u00eas, deployment).</li> </ul> <p>Not so shallow...   </p> <p>Al\u00e9m de descritas abaixo, cada uma dessas etapas cont\u00e9m se\u00e7\u00f5es particulares onde s\u00e3o abordadas com mais detalhes, incluindo estrat\u00e9gias sobre o que fazer em cada momento da etapa e como fazer.</p> <p>A gra\u00e7a do mundo est\u00e1 na diversidade</p> <p>O ciclo de vida apresentado aqui \u00e9 o considerado por Andrew Ng. No entanto, h\u00e1 diversas figuras importantes na \u00e1rea de ML com perspectivas diferentes sobre o ciclo de vida e suas etapas.</p> <p>Por exemplo, este ciclo de vida pode transmitir uma no\u00e7\u00e3o de ser algo direto e n\u00e3o repetitivo. Por\u00e9m, muito pelo contr\u00e1rio, na pr\u00e1tica o processo de cria\u00e7\u00e3o de um modelo de ML \u00e9 extremamente iterativo, ciclo e dificilmente termina. Afinal, uma vez que o modelo est\u00e1 em produ\u00e7\u00e3o, precisamos mant\u00ea-lo! Consequentemente, temos que desenvolver toda uma estrutura para que isso seja feito da forma mais din\u00e2mica poss\u00edvel. Nesse sentido, prepara\u00e7\u00e3o de dados \u00e9 a etapa onde toda a arquitetura de dados para o projeto \u00e9 definida, por exemplo.</p>"},{"location":"machine-learning-engineering/#escopo","title":"Escopo","text":"<p>O escopo \u00e9 a etapa onde o projeto \u00e9 definido. Logo:</p> <ul> <li>Qual problema ser\u00e1 atacado</li> <li>Como o problema ser\u00e1 atacado</li> <li>Qual o crit\u00e9rio de sucesso</li> </ul> <p>Os principais objetivos desta etapa s\u00e3o identificar o que deve ser feito e o qu\u00e3o vi\u00e1vel \u00e9 o que deve ser. Assim, \u00e9 muito importante que o objetivo do projeto seja claro e bem definido.</p> <p>Perguntas que podem nos ajudar a definir o objetivo do projeto s\u00e3o:</p> <ul> <li>Qual \u00e9 o problema que queremos resolver usando ML?</li> <li>Por que queremos aplicar ML?</li> <li>Quais s\u00e3o as entradas que ser\u00e3o consumidas pelo modelo? Quais as sa\u00eddas que devem ser geradas pelo modelo?</li> <li>Quais m\u00e9tricas e crit\u00e9rios definem o sucesso do modelo?</li> <li>Quais m\u00e9tricas e crit\u00e9rios definem o sucesso do projeto?</li> </ul> <p>Nota</p> <p>O objetivo do modelo n\u00e3o necessariamente precisa ser o mesmo objetivo do ponto de vista de neg\u00f3cios (ou seja, o que o cliente busca alcan\u00e7ar).</p> <p>Por exemplo, uma empresa de e-commerce pode ter como objetivo maximizar os lucros com base no pre\u00e7o dos produtos. J\u00e1 o modelo pode ter como objetivo encontrar o pre\u00e7o de um conjunto de produtos que maximize a probabilidade de compra conjunta destes produtos.</p> <p>Outra pr\u00e1tica comum que pode nos ajudar a definir o escopo do projeto com mais rigor (tal como sua execu\u00e7\u00e3o) \u00e9 o uso de Canvas, como o ML Canvas.</p>"},{"location":"machine-learning-engineering/#risco-e-impacto-do-projeto","title":"Risco e Impacto do Projeto","text":"<p>Uma vez definido o que deve ser feito, \u00e9 comum avaliarmos a viabilidade do projeto atrav\u00e9s de estimativas de riscos e impacto.</p> <p>At\u00e9 o momento n\u00e3o existem m\u00e9todos de estima\u00e7\u00e3o de complexidade de um projeto de ML que s\u00e3o amplamente utilizados pela ind\u00fastria. Ainda, projetos de ML s\u00e3o incertos por natureza uma vez que nem sempre os recursos necess\u00e1rios para a solu\u00e7\u00e3o do problema existem ou s\u00e3o vi\u00e1veis.</p> <p>Por exemplo, \u00e9 dif\u00edcil definir com precis\u00e3o quais ser\u00e3o os dados necess\u00e1rios, a quantidade de dados necess\u00e1rios, se os modelos existentes na literatura resolvem o problema em quest\u00e3o, etc.</p> <p>Portanto, o uso de modelos de risco-impacto \u00e9 uma abordagem segura e efetiva. Um exemplo de modelo de risco-impacto para projetos de ML \u00e9 o apresentado na figura abaixo.</p> Fonte: Business Objectives \u2013 ML Life Cycle by ProductizeML"},{"location":"machine-learning-engineering/#custos-do-projeto","title":"Custos do Projeto","text":"<p>O custo de desenvolvimento e opera\u00e7\u00e3o de um projeto de ML \u00e9 um fator decisivo na balan\u00e7a de risco e impacto.</p> <p>De acordo com Andriy Burkov, h\u00e1 tr\u00eas fatores principais que influenciam consideravelmente o custo de um projeto de ML.</p> <ul> <li>Dificuldade do problema. Quanto maior a dificuldade do problema, maior o custo de execu\u00e7\u00e3o e engenharia. Perguntas que nos ajudam a identificar a dificuldade do problema s\u00e3o:<ul> <li>H\u00e1 empresas que j\u00e1 resolveram o mesmo problema ou semelhante?</li> <li>H\u00e1 solu\u00e7\u00f5es para problemas parecidos na academia?</li> <li>H\u00e1 implementa\u00e7\u00f5es dispon\u00edveis de algoritmos capazes de resolver o problema em quest\u00e3o?</li> <li>O quanto de poder computacional \u00e9 necess\u00e1rio para construir e executar o modelo em produ\u00e7\u00e3o?</li> </ul> </li> <li>Custo de aquisi\u00e7\u00e3o dos dados. Coletar a quantidade necess\u00e1ria dos dados corretos tende a ser custoso, principalmente se h\u00e1 necessidade de categoriza\u00e7\u00e3o manual dos dados. Perguntas que nos ajudam a identificar o custo de aquisi\u00e7\u00e3o dos dados s\u00e3o:<ul> <li>Quais os dados necess\u00e1rios?</li> <li>Qual a quantidade de dados neces\u00e1rios?</li> <li>Os dados podem ser gerados automaticamente?</li> <li>\u00c9 necess\u00e1rio categorizar amostras? Se sim, quantas? Qual o custo?</li> </ul> </li> <li>Necessidade de assertividade. Quanto maior a necessidade de assertividade do modelo, maior o custo associado (que crescer\u00e1 exponencialmente). Afinal, a assertividade de um modelo n\u00e3o depende apenas do modelo em si, mas tamb\u00e9m dos dados dispon\u00edveis (geralmente, quanto mais dados, melhor) e dificuldade do problema.</li> </ul> <p>Nota</p> <p>Novamente, aqui o uso de Canvas para ML tamb\u00e9m \u00e9 \u00fatil tanto para fazermos as perguntas certas quanto encontrarmos as respostas corretas e assim estimarmos custos com maior precis\u00e3o.</p>"},{"location":"machine-learning-engineering/#definindo-baselines","title":"Definindo Baselines","text":"<p>Do ponto de vista de um projeto de ML, uma baseline \u00e9 o desempenho base a partir do qual queremos melhorar. Portanto, antes de come\u00e7armos a implementarmos nossos pr\u00f3prios modelos, \u00e9 importante pesquisarmos solu\u00e7\u00f5es j\u00e1 existentes para o problema que queremos atacar (ou ent\u00e3o, semelhantes ao problema que queremos atacar).</p> <p>O Model Zoo, por exemplo, \u00e9 uma plataforma onde diversos modelos pr\u00e9-treinados (de deep learning) s\u00e3o disponibilizados, de forma que o desenvolver tenha apenas que \"plug\u00e1-lo\" no sistema ou processo de desenvolvimento.</p>"},{"location":"machine-learning-engineering/#preparacao-dos-dados","title":"Prepara\u00e7\u00e3o dos Dados","text":"<p>Esta \u00e9 a etapa onde os dados necess\u00e1rios para a execu\u00e7\u00e3o do projeto e constru\u00e7\u00e3o do modelo s\u00e3o coletados e processados.</p> <p>Note que a prepara\u00e7\u00e3o de dados \u00e9 absolutamente importante, visto que erros nos dados s\u00e3o propagados ao longo de todo o projeto, o que pode resultar em problemas cr\u00edticos.</p> <p>Al\u00e9m disso, a etapa de prepara\u00e7\u00e3o de dados \u00e9 (geralmente) composta pelas seguintes tarefas:</p> <ul> <li>Ingest\u00e3o de Dados. Coleta e armazenamento de dados oriundos de diversas fontes em diversos mecanismos de armazenamento, tais como Data Lakes e Data Warehouses. Essa etapa tamb\u00e9m pode incluir o enriquecimento de dados e/ou gera\u00e7\u00e3o de dados sint\u00e9ticos.</li> <li>Explora\u00e7\u00e3o e Valida\u00e7\u00e3o. Perfilamento de dados a fim de obter informa\u00e7\u00f5es sobre sua estrutura que, por sua vez, s\u00e3o utilizadas para definir poss\u00edveis esquemas de dados, assim como rotinas de teste e valida\u00e7\u00e3o.</li> <li>Data Cleaning. Processo de formata\u00e7\u00e3o dos dados e corre\u00e7\u00e3o de erros (e.g. valores faltantes ou inconsistentes) de forma que se enquadrem nos esquemas definidos.</li> <li>Data Splitting. Divis\u00e3o do dados em conjuntos dedicados ao treinamento, valida\u00e7\u00e3o e teste dos modelos produzidos.</li> </ul> <p>Entramos em mais detalhes sobre a prepara\u00e7\u00e3o de dados na se\u00e7\u00e3o Prepara\u00e7\u00e3o de Dados.</p>"},{"location":"machine-learning-engineering/#modelagem","title":"Modelagem","text":"<p>Esta \u00e9 a etapa onde os modelos cogitados para atacar o problema definido no escopo s\u00e3o treinados, testados, selecionados e reavaliados.</p> <p>As subetapas principais da etapa de modelagem s\u00e3o:</p> <ul> <li>Treinamento &amp; Sele\u00e7\u00e3o de Modelos. Processo onde modelos s\u00e3o treinados e o melhor \\(-\\) com base em m\u00e9tricas de desempenho para a tarefa em quest\u00e3o (e.g. acur\u00e1cia, erro quadr\u00e1tico m\u00e9dio, etc) \\(-\\) \u00e9 selecionado como candidato \u00e0 produ\u00e7\u00e3o.</li> <li> <p>Avalia\u00e7\u00e3o. Processo onde s\u00e3o executadas an\u00e1lises de erro a fim de verificar se o modelo \\(-\\) al\u00e9m de possuir boas m\u00e9tricas de desempenho (do ponto de vista de treinamento) \\(-\\) resolve o problema definido. Nesta etapa tamb\u00e9m \u00e9 comum avaliar um poss\u00edvel enviesamento do modelo, assim como requisitos n\u00e3o funcionais (e.g. tempo de infer\u00eancia, custo computacional, etc.)</p> <p>Al\u00e9m disso, \u00e9 neste momento que comparamos o modelo constru\u00eddo com poss\u00edveis baselines e analisamos quais pontos podem ser melhorados.</p> </li> </ul> <p>Entramos em mais detalhes sobre o treinamento e avalia\u00e7\u00e3o de modelos nas se\u00e7\u00f5es Constru\u00e7\u00e3o de Modelos e Avalia\u00e7\u00e3o, respectivamente.</p>"},{"location":"machine-learning-engineering/#implantacao","title":"Implanta\u00e7\u00e3o","text":"<p>Etapa onde o modelo constru\u00eddo \u00e9 colocado em produ\u00e7\u00e3o. Logo, al\u00e9m das pr\u00e1ticas convencionais aplicadas no desenvolvimento de modelos de ML, durante o deployment (implanta\u00e7\u00e3o) pr\u00e1ticas de engenharia de software s\u00e3o aplicadas com mais intensidade, incluindo testes unit\u00e1rios e de integra\u00e7\u00e3o, integra\u00e7\u00e3o do modelo com o restante do sistema, defini\u00e7\u00e3o de pol\u00edticas de monitoramento, etc.</p> <p>A etapa de implanta\u00e7\u00e3o \u00e9 geralmente composta pelos seguintes processos:</p> <ul> <li>Model Serving. Processo de disponibiliza\u00e7\u00e3o do modelo em um ambiente de produ\u00e7\u00e3o para acesso pelos usu\u00e1rios.</li> <li>Monitoramento de Performance. Processo de monitoramento da performance do modelo em dados n\u00e3o vistos anteriormente a fim de identificar poss\u00edveis falhas no seu desenvolvimento e queda de desempenho ao longo do tempo.</li> </ul> <p>Nota</p> <p>Mesmo ap\u00f3s o deployment inicial, a manuten\u00e7\u00e3o de tanto o sistema quanto o modelo \u00e9 necess\u00e1ria. Portanto, \u00e9 comum reexecutarmos etapas anteriores frequentemente.</p> <p>Entramos em mais detalhes sobre implanta\u00e7\u00e3o, serving e monitoramento nas se\u00e7\u00f5es Implanta\u00e7\u00e3o, Model Serving e Monitoramento, respectivamente.</p>"},{"location":"machine-learning-engineering/#referencias","title":"Refer\u00eancias","text":"<ul> <li>Designing Machine Learning Systems by Chip Huyen</li> <li>Machine Learning Engineering by Andriy Burkov</li> <li>Introduction to Machine Learning in Production by Coursera</li> <li>An Overview of the End-to-End Machine Learning Workflow by MLOps</li> <li>ML Life Cycle by ProductizeML</li> </ul>"},{"location":"machine-learning-engineering/baseline-ml-models/","title":"Estabelecendo Baselines","text":""},{"location":"machine-learning-engineering/baseline-ml-models/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>No contexto de projetos de ML, um baseline \u00e9 um modelo simples mas que alcan\u00e7a resultados razo\u00e1veis no problema que desejamos resolver e cujos resultados s\u00e3o utilizados como ponto de partida (ou seja, define um desempenho m\u00ednimo) para o desempenho que queremos alcan\u00e7ar e, consequentemente, a constru\u00e7\u00e3o de modelos mais complexos.</p> <p>Nota</p> <p>Por simples, queremos dizer que \u00e9 um modelo f\u00e1cil de treinar, implantar e que n\u00e3o exige grande expertise. No geral, modelos assim tamb\u00e9m s\u00e3o f\u00e1ceis de explicar e analisar.</p> <p>De fato, definir um ponto de partida de performance (baseline level performance) \u00e9 uma pr\u00e1tica muito importante para a evolu\u00e7\u00e3o de desempenho de um modelo. Afinal, este ponto de partida nos ajuda a definir:</p> <ul> <li>Qual o desempenho m\u00ednimo aceit\u00e1vel para o modelo entrar em produ\u00e7\u00e3o e o qu\u00e3o fact\u00edvel esse desempenho \u00e9.</li> <li>Quais processamentos precisamos fazer nos dados a fim de melhorar sua qualidade.</li> <li>O quanto de recurso computacional (e humano) ser\u00e1 necess\u00e1rio para construir o modelo desejado.</li> </ul> <p>Por exemplo:</p> <ul> <li>Tarefas de Regress\u00e3o. Regress\u00e3o Linear.</li> <li>Tarefas de Classifica\u00e7\u00e3o em Dados Estruturados. K-Nearest Neighbors.</li> <li>Tarefas de Vis\u00e3o Computacional ou NLP. Modelos Pr\u00e9-treinados.</li> </ul>"},{"location":"machine-learning-engineering/baseline-ml-models/#tipos-de-baseline","title":"Tipos de Baseline","text":"<p>Emmanuel Ameisen define quatro tipos de baseline de performance:</p> <ul> <li>Performance Trivialmente Alcan\u00e7\u00e1vel. Desempenho obtido da maneira mais simples poss\u00edvel. \u00c9 esperado que qualquer modelo ultrapasse esse desempenho.</li> <li>Performance de N\u00edvel Humano (HLP, Human Level Performance). Desempenho obtido por humanos na tarefa em quest\u00e3o. Quando um modelo ultrapassa esse desempenho, ent\u00e3o \u00e9 um bom candidato a entrar em produ\u00e7\u00e3o. O uso de HLP \u00e9 recomend\u00e1vel principalmente em tarefas que envolvam a classifica\u00e7\u00e3o de dados n\u00e3o estruturados.</li> <li>Performance Automatizada Razo\u00e1vel. Desempenho obtido por um modelo consideravelmente simples. Esse desempenho nos ajuda a julgar se um modelo complexo est\u00e1 performando bem o suficiente em rela\u00e7\u00e3o a sua complexidade de implementa\u00e7\u00e3o.</li> <li>Performance M\u00ednima para Deployment. Desempenho m\u00ednimo necess\u00e1rio para que um modelo possa entrar em produ\u00e7\u00e3o.</li> </ul> <p>Qual tipo de baseline ser\u00e1 adotada depende do dom\u00ednio do problema e objetivos. Contudo, \u00e9 aconselh\u00e1vel sempre definir uma performance m\u00ednima para deployment, assim como o desempenho obtido por um modelo simples (performance automatizada razo\u00e1vel). Se estivermos trabalhando com dados n\u00e3o-estruturados, podemos incluir HLP.</p>"},{"location":"machine-learning-engineering/baseline-ml-models/#referencias","title":"Refer\u00eancias","text":"<ul> <li>Topic 3 - Baselines</li> <li>Always Start with a Stupid Model, No Exceptions by Emmanuel Ameisen</li> <li>Introduction to Machine Learning in Production by Coursera</li> </ul>"},{"location":"machine-learning-engineering/data-centric-ai/","title":"Data-Centric AI","text":""},{"location":"machine-learning-engineering/data-centric-ai/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>De acordo com Andrew Ng, h\u00e1 duas maneiras que podemos adotar para desenvolver modelos de ML:</p> <ul> <li>Model-centric (centrado no modelo). Estrat\u00e9gia onde dado um conjunto de dados fixo, aplicamos diferentes algoritmos e estrat\u00e9gias para encontrar o melhor modelo para aqueles dados e, consequentemente, o melhor modelo para a tarefa em quest\u00e3o.</li> <li>Data-centric (centrado no dado). Estrat\u00e9gia onde dado um conjunto de dados fixo, aplicamos diferentes estrat\u00e9gias de processamento de dados a fim de torn\u00e1-los o mais representativos poss\u00edvel, de modo que qualquer algoritmo minimamente complexo seja capaz de aprend\u00ea-los e assim performar bem na tarefa em quest\u00e3o.</li> </ul> <p>Embora o uso do Model-centric seja muito comum na academia, na ind\u00fastria \u00e9 fortemente recomendado (principalmente por Andrew Ng) o uso da abordagem Data-centric. Mais precisamente, dado um problema resolv\u00edvel por meio de ML, devemos buscar aumentar ao m\u00e1ximo poss\u00edvel a qualidade dos dados, tornando-os \"bom dados\" (good data), sendo \"good data\":</p> <ul> <li>Definidos consistentemente (a defini\u00e7\u00e3o de cada categoria \u00e9 n\u00e3o-amb\u00edguia)</li> <li>Todos os casos importantes s\u00e3o cobertos (cobertura de todas as possibilidades de entrada)</li> <li>Feedback constante de altera\u00e7\u00f5es (concept drift e data drift s\u00e3o monitorados)</li> <li>Quantidade de dados apropriada.</li> </ul>"},{"location":"machine-learning-engineering/data-centric-ai/#referencias","title":"Refer\u00eancias","text":"<ul> <li>A Chat with Andrew on MLOps: From Model-centric to Data-centric AI</li> <li>Data-centric AI Community</li> </ul>"},{"location":"machine-learning-engineering/experiment-tracking/","title":"Rastreamento e Versionamento de Experimentos","text":""},{"location":"machine-learning-engineering/experiment-tracking/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Uma pr\u00e1tica extremamente importante nos projetos de ML \u00e9 o rastreamento dos experimentos, tal como o versionamento do c\u00f3digo e artefatos produzidos pelo experimento.</p> <p>Ao rastrear e versionar os experimentos, conseguimos:</p> <ul> <li>Acompanhar os resultados de diferentes itera\u00e7\u00f5es de um experimento, visto que cada itera\u00e7\u00e3o retornar\u00e1 valores de m\u00e9tricas diferentes.</li> <li>Acompanhar os resultados alcan\u00e7ados por cada (hiper)par\u00e2metro.</li> <li>Registrar, para cada itera\u00e7\u00e3o, o c\u00f3digo do experimento, assim como os artefatos (i.e. modelo e dados utilizados para construir o modelo).</li> <li>Manter rastreada a exata vers\u00e3o do c\u00f3digo de uma itera\u00e7\u00e3o, tal como o dado e modelo resultante desta vers\u00e3o.</li> </ul> <p>Basicamente, o rastreio de experimentos \u00e9 a pr\u00e1tica de salvar (i.e. \"loggar\") todas as informa\u00e7\u00f5es importantes relacionadas aos dados, modelo e c\u00f3digo de cada itera\u00e7\u00e3o do experimento executado, de forma que seja poss\u00edvel se ter um conhecimento completo de cada informa\u00e7\u00e3o gerada e o controle total sobre todas as modifica\u00e7\u00f5es realizadas.</p> <p>Por exemplo, ao desenvolvermos um modelo, podemos querer rastrear (e versionar) em cada itera\u00e7\u00e3o de um experimento:</p> <ul> <li>Scripts (c\u00f3digo-fonte) usado.</li> <li>Arquivos de configura\u00e7\u00e3o.</li> <li>Dados e metadados utilizados para o treinamento, valida\u00e7\u00e3o e teste.</li> <li>Par\u00e2metros e hiperpar\u00e2metros do modelo.</li> <li>Resultados das m\u00e9tricas de avalia\u00e7\u00e3o do modelo.</li> <li>Resultados das m\u00e9tricas de performance da aplica\u00e7\u00e3o.</li> </ul> <p>Uma vez tendo essas informa\u00e7\u00f5es, podemos:</p> <ul> <li>Comparar os diferentes resultados alcan\u00e7ados</li> <li>Identificar o impacto de cada altera\u00e7\u00e3o no resultado final</li> <li>Identificar problemas de performance do sistema, etc.</li> </ul> <p>Portanto, a pr\u00e1tica de rastrear os experimentos \u00e9 fundamental tanto para a reprodutiblidade (de fato, \u00e9 a principal forma de alcan\u00e7\u00e1-la) quanto para o desenvolvimento da aplica\u00e7\u00e3o em si.</p> <p>Rastrear manualmente experimentos pode ser extremamente complexo. Por isso usamos ferramentas free e open-source dispon\u00edveis, como \u00e9 o caso do MLflow Tracking.</p>"},{"location":"machine-learning-engineering/experiment-tracking/#mlflow-tracking","title":"MLflow Tracking","text":"<p>O MLflow Tracking \u00e9 uma ferramenta para loggar par\u00e2metros, m\u00e9tricas, vers\u00f5es de c\u00f3digo e artefatos, arquivos de dados, etc. Podemos loggar os experimentos atrav\u00e9s da API Python, REST API e CLI. Ainda, a ferramenta tamb\u00e9m fornece uma UI que nos permite visualizar e comparar os resultados.</p> <p>O MLflow Tracking gira em torno do conceito de runs.</p> <p>Uma run nada mais \u00e9 que a execu\u00e7\u00e3o de um peda\u00e7o de c\u00f3digo (por isso, usaremos \"run\" e execu\u00e7\u00e3o de forma intercambi\u00e1vel).</p> <p>No caso, usamos tal comportamento para definirmos itera\u00e7\u00f5es de um experimento. Assim, cada run \u00e9 uma execu\u00e7\u00e3o de um experimento com um certo conjunto de par\u00e2metros.</p> <p>Para cada run s\u00e3o salvos:</p> <ul> <li><code>Code version</code>. Hash do commit (Git) usado na execu\u00e7\u00e3o (se o experimento estiver em um reposit\u00f3rio).</li> <li><code>Start &amp; End time</code>. Hora de in\u00edcio e t\u00e9rmino da run.</li> <li><code>Source</code>. Nome do arquivo (source code) de registro do experimento.</li> </ul> <p>Ainda, s\u00e3o salvos outros metadados \u00fateis para a ferramenta.</p> <p>Dica</p> <p>Se as execu\u00e7\u00f5es estiverem organizadas em um MLflow Project, tamb\u00e9m s\u00e3o registrados o URI do projeto e a vers\u00e3o do c\u00f3digo-fonte.</p> <p>Para acompanhar os experimento rastreados, utilize a UI invocando o web server:</p> <pre><code>$ mlflow ui\n</code></pre> <p>Aten\u00e7\u00e3o</p> <p>A UI deve ser invocada no mesmo diret\u00f3rio onde os dados dos experimentos s\u00e3o logados. Veja a se\u00e7\u00e3o Como Execu\u00e7\u00f5es s\u00e3o Armazenadas</p>"},{"location":"machine-learning-engineering/experiment-tracking/#criando-experimentos","title":"Criando Experimentos","text":"<p>Uma vez que execu\u00e7\u00f5es s\u00e3o agrupadas em experimentos, para cri\u00e1-los utilizamos o m\u00e9todo <code>set_experiment()</code>.</p> <ul> <li><code>set_experiment(experiment_name)</code>. Atribui o experimento passado por par\u00e2metro como o experimento ativo. Caso o experimento n\u00e3o exista, ent\u00e3o ele \u00e9 criado.</li> </ul> <pre><code>import mlflow\n\nmlflow.set_experiment(\"Experiment #1\")\n</code></pre>"},{"location":"machine-learning-engineering/experiment-tracking/#criando-execucoes","title":"Criando Execu\u00e7\u00f5es","text":"<p>Uma vez definido o experimento, precisamos definir a itera\u00e7\u00e3o de execu\u00e7\u00e3o do experimento a partir do qual as entidades (i.e. par\u00e2metros, m\u00e9tricas, etc.) e artefatos (i.e. modelo e dados) ser\u00e3o logados.</p> <p>Podemos definir itera\u00e7\u00f5es atrav\u00e9s da chamada simples dos m\u00e9todos <code>mlflow.start_run()</code> e <code>mlflow.end_run()</code> ou usando <code>mlflow.start_run()</code> como um gerenciador de contexto.</p> <ul> <li><code>mlflow.start_run(run_id, experiment_id, run_name, nested, tags)</code>. Caso exista uma execu\u00e7\u00e3o ativa, ent\u00e3o a retorna. Caso contr\u00e1rio, cria uma nova itera\u00e7\u00e3o.</li> <li> <p><code>mlflow.end_run()</code>. Encerra uma execu\u00e7\u00e3o ativa, caso exista.</p> <pre><code>mlflow.start_run()\nmlflow.log_param(\"param\", 1.0)\nmlflow.end_run()\n</code></pre> </li> </ul> <p>Contudo, o uso mais recomendado dos m\u00e9todos \u00e9 usando gerenciadores de contexto.</p> <pre><code>with mlflow.start_run() as run:\n    mlflow.log_param(\"param\", 1.0)\n</code></pre>"},{"location":"machine-learning-engineering/experiment-tracking/#parametros","title":"Par\u00e2metros","text":"<ul> <li><code>run_id</code>. Executa a itera\u00e7\u00e3o sobre uma execu\u00e7\u00e3o passada cujo identificador \u00e9 <code>run_id</code> (UUID)</li> <li><code>experiment_id.</code> Executa a itera\u00e7\u00e3o sob um experimento cujo identificador \u00e9 <code>experiment_id</code>.</li> <li><code>run_name</code>. Nome da execu\u00e7\u00e3o (salvo na tag/arquivo <code>mlflow.runName</code>).</li> </ul>"},{"location":"machine-learning-engineering/experiment-tracking/#boas-praticas","title":"Boas Pr\u00e1ticas","text":"<p>Sempre que o c\u00f3digo referente a itera\u00e7\u00e3o de um experimento \u00e9 executado, \u00e9 criado um novo diret\u00f3rio (cujo nome \u00e9 o <code>run_id</code>, na forma de UUID) onde as entidades e artefatos s\u00e3o armazenados.</p> <p>Logo, \u00e9 importante manter o <code>run_name</code> consistente, pois ao acessarmos a UI poder\u00e1 haver v\u00e1rias execu\u00e7\u00f5es com o mesmo <code>run_name</code>.</p>"},{"location":"machine-learning-engineering/experiment-tracking/#loggando-entidades-e-artefatos","title":"Loggando Entidades e Artefatos","text":"<p>Cada execu\u00e7\u00e3o (i.e. run) \u00e9 registrada na forma de um diret\u00f3rio. Dentro do diret\u00f3rio h\u00e1 um arquivo de metadados denominado <code>meta.yml</code> e quatro diret\u00f3rios:</p> <ul> <li><code>params/</code>. Diret\u00f3rio onde s\u00e3o armazenados os par\u00e2metros logados.</li> <li><code>metrics/</code>. Diret\u00f3rio onde s\u00e3o armazenadas as m\u00e9tricas logadas.</li> <li><code>artifacts/</code>. Diret\u00f3rio onde s\u00e3o armazenados os artefatos.</li> <li><code>tags/</code>. Diret\u00f3rio onde s\u00e3o armazenadas as tags.</li> </ul> <p>No caso dos par\u00e2metros e m\u00e9tricas, cada par\u00e2metro indicado pelo argumento <code>key</code> \u00e9 um arquivo de texto cujo conte\u00fado \u00e9 a <code>key</code> e o respectivo valor definido.</p> <p>Os m\u00e9todos de logging mais importantes s\u00e3o:</p> <ul> <li><code>log_param(key, value)</code>. Loga um parametro de nome <code>key</code> e valor <code>value</code></li> <li> <p><code>log_params(dict)</code>. Semelhante a <code>log_param</code>, mas loga um conjunto de par\u00e2metros organizados em um dicion\u00e1rio.</p> <p>Aten\u00e7\u00e3o!</p> <p>N\u00e3o \u00e9 poss\u00edvel logar o mesmo par\u00e2metro mais de uma vez usando o m\u00e9todo <code>log_param</code>. Caso isso seja necess\u00e1rio, use <code>log_params</code>.</p> <p>Ainda, \u00e9 recomendado o uso consistente dos m\u00e9todos. Logo, opte por sempre usar um ou outro.</p> </li> <li> <p><code>log_metric(key, value, step)</code>. An\u00e1logo ao <code>log_param</code> com a diferen\u00e7a do argumento <code>step</code>, que nos permite logar v\u00e1rios resultados para uma mesma m\u00e9trica (assim, podemos registrar experimentos executados com Valida\u00e7\u00e3o Cruzada, por exemplo).</p> </li> <li><code>log_metrics(dict, step)</code>. An\u00e1logo ao <code>log_metric</code> mas para conjuntos de m\u00e9tricas.</li> <li><code>log_artifact(local_path, artifact_path)</code>.</li> <li><code>log_artifacts(local_dir, artifact_path)</code>.</li> </ul>"},{"location":"machine-learning-engineering/experiment-tracking/#loggando-modelos","title":"Loggando Modelos","text":"<p>TODO</p>"},{"location":"machine-learning-engineering/experiment-tracking/#boas-praticas_1","title":"Boas Pr\u00e1ticas","text":"<p>Por padr\u00e3o, ao loggar modelos, apenas o bin\u00e1rio do modelo e as depend\u00eancias para sua execu\u00e7\u00e3o s\u00e3o armazenadas.</p> <p>Por\u00e9m, \u00e9 interessante manter junto ao modelo os dados utilizados para sua constru\u00e7\u00e3o. Afinal, c\u00f3digo, dado e modelo precisam estar sempre sincronizados.</p> <p>Dessa forma, pr\u00e1ticas que podemos adotar s\u00e3o:</p> <ol> <li> <p>(Recomendada) Loggar os dados como artefatos. A primeira e mais simples alternativa \u00e9 salvar os dados como artefatos junto do modelo.</p> <pre><code># [...]\n# df = pd.read_csv(\"path/to/dataset.csv\")\n\nmlflow.log_artifact(local_path=\"path/to/dataset.csv\")\n</code></pre> <p>O problema dessa abordagem \u00e9 que tanto as itera\u00e7\u00f5es quanto o armazenamento em si pode ficar custoso.</p> <p>Podemos amenizar o custo de armazenamento, ao loggarmos tanto dado quanto modelo em objects storage (e.g. Hadoop ou AWS S3).</p> <p>Contudo, o custo de upload do dado ainda tende a ser custoso.</p> </li> <li> <p>(N\u00e3o recomendada) Usar uma ferramenta de versionamento de dado, tal como DVC. Embora interessante a princ\u00edpio, essa estrat\u00e9gia possui v\u00e1rias complica\u00e7\u00f5es*.</p> </li> </ol>"},{"location":"machine-learning-engineering/experiment-tracking/#como-execucoes-entidades-e-artefatos-sao-armazenados","title":"Como Execu\u00e7\u00f5es (Entidades e Artefatos) s\u00e3o Armazenados","text":"<p>As execu\u00e7\u00f5es (runs) do MLflow Tracking podem ser armazenadas no sistema local de arquivos (como arquivos locais), bancos de dados compat\u00edveis com SQLAlchemy ou remotamente para um servi\u00e7o de tracking.</p> <p>J\u00e1 os artefatos podem ser armazenados tanto localmente quanto em diversos servi\u00e7os de armazenamento de arquivos.</p> <p>O MLflow utiliza dois componentes para o armazenamento dos dados:</p> <ul> <li>Backend Store. Armazena as entidades MLflow (metadados de execu\u00e7\u00e3o, par\u00e2metros, m\u00e9tricas, tags, notas, etc.)</li> <li>Artifact Store. Armazena os artefatos (arquivos, modelos, dados, imagens, objetos, etc.)</li> </ul> <p>Para definirmos como e onde as execu\u00e7\u00f5es ser\u00e3o registradas, usamos o m\u00e9todo <code>set_tracking_uri</code>.</p>"},{"location":"machine-learning-engineering/experiment-tracking/#armazenando-no-sistema-de-arquivos-local","title":"Armazenando no Sistema de Arquivos Local","text":"<p>Ao utilizar o sistema de arquivos local, ambos os componentes backend store e artifact store armazenam os dados no diret\u00f3rio <code>./mlruns</code>. As interfaces para tal s\u00e3o:</p> <ul> <li><code>LocalArtifactRepository</code> para armazenar os artefatos.</li> <li><code>FileStore</code> para armazenar as entidades.</li> </ul> <p>Assim, para cada experimento:</p> <ol> <li> <p>MLflow cria um diret\u00f3rio (cujo nome s\u00e3o n\u00fameros inteiros) dentro de <code>./mlruns</code> para armazenar as execu\u00e7\u00f5es. Por padr\u00e3o, o MLflow sempre criar\u00e1 o diret\u00f3rio <code>0</code>. Este diret\u00f3rio \u00e9 de prop\u00f3sito geral e utilizado para armazenar as execu\u00e7\u00f5es que n\u00e3o organizadas em experimentos.</p> </li> <li> <p>Cada execu\u00e7\u00e3o possui um diret\u00f3rio dentro da pasta do respectivo experimento onde s\u00e3o armazenados os artefatos e entidades. O nome do diret\u00f3rio de cada execu\u00e7\u00e3o \u00e9 o UUID da respectiva execu\u00e7\u00e3o.</p> </li> </ol> <p>Nota</p> <p>Ao especificarmos um caminho para o sistema de arquivos local, devemos sempreutilizar a sintaxe: <code>file:///path/to/dir/mlflow/</code>.</p> <p>Isso porque caso a gente n\u00e3o adicione o diret\u00f3rio <code>mlflow</code> no final dasintaxe, n\u00e3o ser\u00e3o criados os diret\u00f3rios <code>mlruns/</code> e nem <code>mlruns/0/</code>.Consequentemente:     - A UI do MLflow n\u00e3o ir\u00e1 conseguir rastrear as execu\u00e7\u00f5es.     - N\u00e3o haver\u00e1 onde adicionar execu\u00e7\u00f5es que n\u00e3o estiverem organizadas emexperimentos.</p> <p>Aten\u00e7\u00e3o</p> <p>Se a URI de rastreamento n\u00e3o for definida \\(-\\) seja atrav\u00e9s da vari\u00e1vel de ambiente <code>MLFLOW_TRACKING_URI</code> ou <code>set_tracking_uri</code> \\(-\\) o MLflow automaticamente ir\u00e1 criar o diret\u00f3rio <code>mlruns/</code> na mesma pasta onde o script Python for chamado.</p>"},{"location":"machine-learning-engineering/experiment-tracking/#versionamento-com-git-e-mlflow-tracking","title":"Versionamento com Git e MLflow Tracking","text":"<p>Al\u00e9m do MLflow Tracking, tamb\u00e9m podemos utilizar o MLflow Project.</p> <p>O MLflow Project \u00e9 um formato de empacotamento de projetos orientados a dados de forma que sejam reutiliz\u00e1veis e reprodut\u00edveis.</p> <p>Essencialmente, um projeto no formato MLflow Project \u00e9 apenas um diret\u00f3rio ou reposit\u00f3rio Git com os arquivos relacionados ao projeto e um arquivo adicionado denominado MLproject onde s\u00e3o definidas as configura\u00e7\u00f5es do projeto permitindo-o que seja execut\u00e1vel.</p> <p>Ainda, no caso de um projeto a partir de um reposit\u00f3rio Git o MLflow associa a vers\u00e3o de cada c\u00f3digo fonte a hash do \u00faltimo commit em que o c\u00f3digo foi adicionado ou modificado.</p> <p>Portanto, ao usar o Git com o MLflow Tracking podemos (sem necessariamente adicionar o arquivo MLproject, cerne do  MLflow Project) logar as entidades e artefatos de cada execu\u00e7\u00e3o de um experimento de forma que o c\u00f3digo, dado e modelos estejam sincronizados.</p>"},{"location":"machine-learning-engineering/experiment-tracking/#boas-praticas_2","title":"Boas Pr\u00e1ticas","text":"<p>Dado que a vers\u00e3o de cada c\u00f3digo fonte (e consequentemente, dos artefatos relacionados) \u00e9 definida com base na hash de um commit, o versionamento correto \u00e9 responsabilidade total do usu\u00e1rio, o que pode ser sujeito a erros.</p> <p>Por exemplo, podemos modificar o c\u00f3digo de um experimento e execut\u00e1-lo diversas vezes sem \"commit\u00e1-lo\". Com isso, v\u00e1rias itera\u00e7\u00f5es ser\u00e3o registradas para um mesmo arquivo de c\u00f3digo fonte (assim como modelo e dado). Por\u00e9m, embora todos tenham a mesma vers\u00e3o, o conte\u00fado de cada execu\u00e7\u00e3o \u00e9 diferente. Dessa forma, fica imposs\u00edvel identificar o c\u00f3digo que gerou os artefatos da respectiva execu\u00e7\u00e3o.</p> <p>Portanto, as pr\u00e1ticas que podemos adotar s\u00e3o:</p> <ol> <li>Sempre que o c\u00f3digo do experimento for alterado, ele deve ser commitado antes de ser executado novamente.</li> <li>Utilizar uma ferramenta para auto-commit (abordagem recomendada). Com isso, toda altera\u00e7\u00e3o permanecer\u00e1 rastreada. Ao mesmo tempo, h\u00e1 um potencial de polui\u00e7\u00e3o do hist\u00f3rico de commits, visto que altera\u00e7\u00f5es pequenas ou pouco significantes tamb\u00e9m ser\u00e3o inclu\u00eddas como commits individuais.</li> <li> <p>Considerar apenas a \u00faltima execu\u00e7\u00e3o.</p> <p>Ao optar por essa abordagem, caso a nova execu\u00e7\u00e3o tenha resultados inferiores, n\u00e3o ser\u00e1 poss\u00edvel reverter o c\u00f3digo para a \u00faltima vers\u00e3o (uma vez que ele n\u00e3o foi versionado)</p> </li> </ol>"},{"location":"machine-learning-engineering/ml-model-development-challenges/","title":"Desafios no Desenvolvimento de Modelos","text":""},{"location":"machine-learning-engineering/ml-model-development-challenges/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Desenvolver modelos tende a ser uma tarefa d\u00edficil. Contudo, n\u00e3o pelo treinamento ou escolha de (hiper)par\u00e2metros, mas sim por conta dos dados dispon\u00edveis e m\u00e9tricas de neg\u00f3cio que queremos atingir.</p>"},{"location":"machine-learning-engineering/ml-model-development-challenges/#principais-problemas","title":"Principais Problemas","text":"<p>De acordo com Andrew Ng, os principais problemas enfretados no desenvolvimento (e manuten\u00e7\u00e3o) de modelos de ML s\u00e3o:</p> <ul> <li>Desempenho do modelo em desenvolvimento x em produ\u00e7\u00e3o</li> <li>M\u00e9tricas de desenvolvimento x de neg\u00f3cios</li> <li>Dados desbalanceados</li> </ul>"},{"location":"machine-learning-engineering/ml-model-development-challenges/#desempenho-no-desenvolvimento-x-em-producao","title":"Desempenho no Desenvolvimento x Em Produ\u00e7\u00e3o","text":"<p>O erro do modelo nas parti\u00e7\u00f5es de valida\u00e7\u00e3o e teste, geralmente, \u00e9 pouco informativo quanto \u00e0 performance do modelo. De fato, a performance real de um modelo aparece somente quando este vai para produ\u00e7\u00e3o, pois apenas a partir deste momento \u00e9 que o modelo est\u00e1 exposto a dados reais.</p> <p>Consequentemente, s\u00e3o necess\u00e1rias estrat\u00e9gias para atualizar rapidamente o modelo em produ\u00e7\u00e3o, tal como experiment\u00e1-lo em produ\u00e7\u00e3o antes de que seja disponibilizado totalmente para todos os usu\u00e1rios.</p>"},{"location":"machine-learning-engineering/ml-model-development-challenges/#metrica-de-desenvolvimento-x-metrica-de-negocios","title":"M\u00e9trica de Desenvolvimento x M\u00e9trica de Neg\u00f3cios","text":"<p>M\u00e9tricas de desenvolvimento s\u00e3o m\u00e9tricas comuns utilizadas para testar a performance de um modelo durante seu desenvolvimento, como: acur\u00e1cia, precis\u00e3o, revoca\u00e7\u00e3o, etc.</p> <p>J\u00e1 m\u00e9tricas de neg\u00f3cios s\u00e3o m\u00e9tricas que indicam precisamente o qu\u00e3o bem (ou eficiente) um problema em quest\u00e3o est\u00e1 sendo resolvido atrav\u00e9s de uma estrat\u00e9gia espec\u00edfica.</p> <p>No geral, tais m\u00e9tricas acabam sendo KPIs. Ou seja, indicadores-chaves de desempenho que medem quantitativamente o qu\u00e3o eficiente est\u00e1 sendo uma estrat\u00e9gia espec\u00edfica (no caso, modelos de IA) em agregar valor ao neg\u00f3cio.</p> <p>Por serem medirem aspectos totalmente diferentes, nem sempre o modelo com melhor acur\u00e1cia \u00e9 aquele que possui melhor KPI. Consequentemente, desenvolver um modelo que atenda com sucesso ambas m\u00e9tricas \u00e9 uma tarefa consideravelmente complexa.</p>"},{"location":"machine-learning-engineering/ml-model-development-challenges/#dados-desbalanceados","title":"Dados Desbalanceados","text":"<p>Dados desbalanceados \u00e9 um dos problemas mais comuns no desenvolvimento de modelos. Desde classes desbalanceadas at\u00e9 distribui\u00e7\u00f5es consideravelmente enviesadas, \u00e9 muito dif\u00edcil tratar conjuntos de dados desbalanceados, pois h\u00e1 muita informa\u00e7\u00e3o sobre certo \"peda\u00e7o\" do conjunto de dados, enquanto h\u00e1 pouca informa\u00e7\u00e3o sobre qualquer outro \"peda\u00e7o\".</p> <p>Embora existam t\u00e9cnicas reamostragem a fim de minimizar os impactos do desbalanceamento, no geral elas s\u00e3o incapazes de produzir modelos confi\u00e1veis. Afinal, como podemos garantir que:</p> <ul> <li>O modelo desfavore\u00e7a pessoas por conta de caracter\u00edstica \u00e9tnico-raciais?</li> <li>As classes minorit\u00e1rias representam corretamente o mundo real?</li> <li>Os dados n\u00e3o possuem tendenciosidades impl\u00edcitas?</li> </ul> <p>De fato, o principal problema da constru\u00e7\u00e3o de modelos de IA na ind\u00fastria s\u00e3o os dados.</p>"},{"location":"machine-learning-engineering/ml-model-development-challenges/#concept-drift-e-data-drift","title":"Concept Drift e Data Drift","text":"<p>Work in progress</p>"},{"location":"machine-learning-engineering/ml-model-development-challenges/#referencias","title":"Refer\u00eancias","text":"<ul> <li>Introduction to Machine Learning in Production by Coursera</li> </ul>"},{"location":"machine-learning-engineering/mlops/","title":"MLOps","text":"<p>Attention! Aten\u00e7\u00e3o!</p> <p>(en_US) This post is a translation (into pt_BR) of my post Introducing MLOps originally published on Daitan's blog.</p> <p>(pt_BR) Esta publica\u00e7\u00e3o \u00e9 uma tradu\u00e7\u00e3o (para pt_BR) do meu artigo Introducing MLOps publicado originalmente no blog da Daitan.</p> <p>Ci\u00eancia de dados e aprendizado de m\u00e1quina (ML, do ingl\u00eas machine learning) t\u00eam se tornado estrat\u00e9gias priorit\u00e1rias na resolu\u00e7\u00e3o de diversos problemas complexos do mundo real.</p> <p>Por\u00e9m, embora implementar e treinar modelos de ML n\u00e3o sejam tarefas triviais, ainda assim</p> <ul> <li>O verdadeiro desafio n\u00e3o \u00e9 construir modelos de ML</li> <li>O verdadeiro desafio \u00e9 construir sistemas de ML integrados e que podem ser atulizados e operados continuamente em produ\u00e7\u00e3o.</li> </ul> <p>Afinal, para tirarmos m\u00e1ximo proveito de um modelo ML, precisamos coloc\u00e1-lo em produ\u00e7\u00e3o.</p> <p>Contudo, de acordo com o relat\u00f3rio \"2020 State of Enterprise Machine Learning\" da Algorithmia.</p> <ul> <li>A maior parte das empresas ainda n\u00e3o descobriram como atingir seus objetivos de ML/IA pois a lacuna entre a constru\u00e7\u00e3o do modelo de ML e o deploy \u00e9 desafiadora.</li> <li>Apenas 22% das companhias que usam aprendizado de m\u00e1quina implantaram com sucesso um modelo de ML em produ\u00e7\u00e3o.</li> </ul> <p>Ao mesmo tempo, a pr\u00f3pria constru\u00e7\u00e3o do modelo e avalia\u00e7\u00e3o deste em escala \u00e9 uma tarefa complicada.</p> <ul> <li>Em aplica\u00e7\u00f5es do \"mundo real\", avaliar o desempenho e impacto do modelo no problema que se busca resolver vai muito al\u00e9m de uma simples experimenta\u00e7\u00e3o de \"treino e teste\".</li> <li>Tamb\u00e9m \u00e9 necess\u00e1rio levar em conta quest\u00f5es como complexidade do algoritmo, velocidade de infer\u00eancia e enviesamento.</li> </ul> <p>Al\u00e9m disso, de acordo com o artigo \"Hidden Technical Debt in Machine Learning Systems\"</p> <ul> <li>Apenas uma pequena fra\u00e7\u00e3o dos sistemas de ML do mundo real \u00e9 composta por c\u00f3digo de ML, enquanto que a infraestrutura envolvente necess\u00e1ria \u00e9 vasta e complexa.</li> </ul> <p></p> <p> Fonte: Sculley, David, et al. \"Hidden technical debt in machine learning systems.\" Advances in neural information processing systems 28 (2015): 2503-2511. </p> <p>Portanto, \u00e9 necess\u00e1rio estabelecer um conjunto de pr\u00e1ticas e processos eficazes para projetar, construir e implantar modelos de ML em produ\u00e7\u00e3o.</p>"},{"location":"machine-learning-engineering/mlops/#definicoes","title":"Defini\u00e7\u00f5es","text":"<p>De acordo com a MLOps SIG, MLOps \u00e9:</p> <p>Cita\u00e7\u00e3o</p> <p>\"A extens\u00e3o da metodologia DevOps para incluir ativos de aprendizado de m\u00e1quina e ci\u00eancia de dados como cidad\u00e3os de primeira classe dentro da ecologia DevOps.\"</p> <p>Por\u00e9m, como uma \u00e1rea em ascens\u00e3o, o termo MLOps n\u00e3o \u00e9 estritamente definido, especialmente quando comparado com machine learning engineering (MLE). Portanto, a defini\u00e7\u00e3o de Andriy Burkov sobre MLE tamb\u00e9m \u00e9 aplic\u00e1vel \u00e0 MLOps, onde</p> <p>Cita\u00e7\u00e3o</p> <p>\"Machine learning engineering \u00e9 o uso de princ\u00edpios cient\u00edficos, ferramentas e t\u00e9cnicas de aprendizado de m\u00e1quina e engenharia de software tradicional para projetar e construir sistemas de computa\u00e7\u00e3o complexos. O MLE abrange todas as etapas, desde a coleta de dados, at\u00e9 a constru\u00e7\u00e3o do modelo, a fim de disponibilizar o modelo para uso pelo produto ou  consumidores.\" \\(\u2014\\) Andriy Burkov</p> <p>Assim, independente do termo utilizado (MLOps e MLE), o que importa \u00e9 o objetivo da \u00e1rea de fornecer um processo de projeto e desenvolvimento de sistemas baseados em machine learning que sejam reprodut\u00edveis, escal\u00e1veis e robustos.</p>"},{"location":"machine-learning-engineering/mlops/#beneficios-do-mlops","title":"Benef\u00edcios do MLOps","text":"<p>Como dito, MLOps tem como objetivo fornecer um conjunto de pr\u00e1ticas e processos eficazes para projetar, construir e implantar modelos escal\u00e1veis de ML em produ\u00e7\u00e3o. Isso pode ser alcan\u00e7ado ao garantir capacidades e qualidades fundamentais, tanto para a aplica\u00e7\u00e3o quanto para o projeto em si. Alguns exemplos s\u00e3o:</p> <ul> <li>Redu\u00e7\u00e3o do d\u00e9bito t\u00e9cnico ao longo do projeto de ML.</li> <li>Aplica\u00e7\u00e3o de Princ\u00edpios \u00c1geis ao projeto de ML.</li> <li>Garantia de reprodutibilidade.</li> <li>Versionamento de dados, pipelines e modelos.</li> <li>Teste automatizando de artefatos de ML.<sup>1</sup></li> <li>Monitoramento de performance dos modelos em produ\u00e7\u00e3o.</li> <li>Suporte a CI/CD para artefatos de ML, incluindo dados.</li> <li>Suporte a CT (Continuous training) para modelos e pipelines.</li> <li>Unifica\u00e7\u00e3o do ciclo de entrega tanto para os modelos quanto para toda a aplica\u00e7\u00e3o.</li> <li>Escalabilidade, alta disponibilidade, toler\u00e2ncia \u00e0 falhas, equidade e seguran\u00e7a no contexto de ML.</li> </ul> <p>Note que a partir dessas capacidades, mais benef\u00edcios surgem, como velocidade no processo de introdu\u00e7\u00e3o dos modelos \u00e0 produ\u00e7\u00e3o, custo reduzido de desenvolvimento e opera\u00e7\u00f5es (em n\u00edvel empresariaral), mitiga\u00e7\u00e3o de riscos associados ao projeto, etc.</p>"},{"location":"machine-learning-engineering/mlops/#praticas-fundamentais","title":"Pr\u00e1ticas Fundamentais","text":"<p>No mundo de MLOps, novas tend\u00eancias e pr\u00e1ticas surgem o tempo todo. Por\u00e9m, h\u00e1 algumas pr\u00e1ticas essenciais que fazem parte do cora\u00e7\u00e3o do MLOps. Tais pr\u00e1ticas s\u00e3o obrigat\u00f3rias para se alcan\u00e7ar um processo de desenvolvimento de sistemas baseados em ML poderoso. Al\u00e9m disso, cada uma dessas pr\u00e1ticas podem ser estendidas e melhoradas.</p>"},{"location":"machine-learning-engineering/mlops/#controle-de-versao","title":"Controle de Vers\u00e3o","text":"<p>Diferente do desenvolvimento convencional de software, aplica\u00e7\u00f5es baseadas em ML possuem tr\u00eas artefatos que devem ser trabalhados: dado, modelo e c\u00f3digo<sup>2</sup>.</p> <p>A pr\u00e1tica de versionar dado, modelo e c\u00f3digo \u00e9 uma extremamente importante no \u00e2mbito de MLOps, visto que a partir do versionamento \u00e9 poss\u00edvel melhorar a reprodutibilidade e garantir manutenibilidade, preven\u00e7\u00e3o de erros e recupera\u00e7\u00e3o de desastres para todo o projeto.</p> <p>Por exemplo, pode haver situa\u00e7\u00f5es onde a atualiza\u00e7\u00e3o de um modelo em produ\u00e7\u00e3o prejudica a performance da aplica\u00e7\u00e3o como um todo. Assim, \u00e9 necess\u00e1rio reverter o modelo (i.e. rollback) para uma vers\u00e3o anterior de modo autom\u00e1tico. Outro caso \u00e9 a necessidade de um tracking pesado de altera\u00e7\u00f5es, uma vez que tanto o dado quanto o modelo s\u00e3o atualizados frequentemente de forma autom\u00e1tica.</p> <p>Assim, o versionamento de artefatos em projetos de ML permite:</p> <ul> <li>Manter tanto altera\u00e7\u00f5es no modelo quanto no dado rastreadas, possibilitando identificar inser\u00e7\u00e3o de bugs ou mudan\u00e7as que ferem a performance da aplica\u00e7\u00e3o.</li> <li>Reverter a vers\u00e3o do modelo para uma vers\u00e3o anterior no caso de releases quebradas (ou que podem vir a quebrar em produ\u00e7\u00e3o).</li> <li>Automatizar todo o pipeline de ML atrav\u00e9s de CI/CD e CT.</li> </ul>"},{"location":"machine-learning-engineering/mlops/#rastreamento-de-experimentos","title":"Rastreamento de Experimentos","text":"<p>Devido a natureza experimental e iterativa de modelos de ML, manter um rastreamento sistem\u00e1\u00feico de todas as informa\u00e7\u00f5es relacionadas aos experimentos \u00e9 essencial. Basicamente, o rastreio de experimentos \u00e9 a pr\u00e1tica de salvar (i.e. \"loggar\") todas as informa\u00e7\u00f5es importantes relacionadas aos dados, modelo e c\u00f3digo de cada itera\u00e7\u00e3o do experimento executado, de forma que seja poss\u00edvel se ter um conhecimento completo de cada informa\u00e7\u00e3o gerada e o controle total sobre todas as modifica\u00e7\u00f5es realizadas.</p> <p>Por exemplo, ao desenvolvermos um modelo, podemos querer rastrear (e versionar) em cada itera\u00e7\u00e3o de um experimento:</p> <ul> <li>Scripts (c\u00f3digo-fonte) usado.</li> <li>Arquivos de configura\u00e7\u00e3o.</li> <li>Dados e metadados utilizados para o treinamento, valida\u00e7\u00e3o e teste.</li> <li>Par\u00e2metros e hiperpar\u00e2metros do modelo.</li> <li>Resultados das m\u00e9tricas de avalia\u00e7\u00e3o do modelo.</li> <li>Resultados das m\u00e9tricas de performance da aplica\u00e7\u00e3o.</li> </ul> <p>Uma vez tendo essas informa\u00e7\u00f5es, podemos comparar os diferentes resultados alcan\u00e7ados, identificar o impacto de cada altera\u00e7\u00e3o no resultado final, identificar problemas de performance do sistema, etc. Portanto, a pr\u00e1tica de rastrear os experimentos \u00e9 fundamental tanto para a reprodutiblidade (de fato, \u00e9 a principal forma de alcan\u00e7\u00e1-la) quanto para o desenvolvimento da aplica\u00e7\u00e3o em si.</p>"},{"location":"machine-learning-engineering/mlops/#pipelines-de-ml-automatizados","title":"Pipelines de ML Automatizados","text":"<p>A automa\u00e7\u00e3o \u00e9 outra pr\u00e1tica fundamental em MLOps. No contexto de ML, a automa\u00e7\u00e3o consiste em automatizar todos os pipelines do workflow de ML, incluindo pipelines de dados, constru\u00e7\u00e3o de modelos e integra\u00e7\u00e3o de c\u00f3digo a fim de que todo o processo seja executado sem qualquer interven\u00e7\u00e3o humana. Com isso:</p> <ul> <li>Os experimentos acontecem de forma mais r\u00e1pida e com uma maior prontid\u00e3o para mover todo o pipeline do desenvolvimento \u00e0 produ\u00e7\u00e3o.</li> <li>Os modelos em produ\u00e7\u00e3o s\u00e3o automaticamente retreinados por meio dos dados atualizados (onde, o retreinamento \u00e9 automaticamente ativado atrav\u00e9s de triggers).</li> <li>O pipeline implementado no ambiente de desenvolvimento \u00e9 correspondente ao utilizado nos ambientes de (pr\u00e9-)produ\u00e7\u00e3o.</li> <li>O pipeline em produ\u00e7\u00e3o est\u00e1 sempre atualizado, uma vez que a etapa de deployment do modelo tamb\u00e9m \u00e9 automatizado.</li> </ul> <p>Logo, considerando um pipeline t\u00edpico de ML, que parte da coleta de dados at\u00e9 a disponibiliza\u00e7\u00e3o do modelo, podemos considerar (no geral) 3 n\u00edveis de automa\u00e7\u00e3o.</p>"},{"location":"machine-learning-engineering/mlops/#nivel-1-processo-manual","title":"N\u00edvel 1 - Processo Manual","text":"<p>O N\u00edvel 1 (Processo Manual) \u00e9 o processo tradicional de ci\u00eancia de dados, onde cada etapa do pipeline \u00e9 executado usando ferramentas RAD (do ingl\u00eas, Rapid Application Development), como Jupyter Notebooks.</p> <p>Este n\u00edvel de automa\u00e7\u00e3o \u00e9 caracterizado principalmente pela natureza experimental e iterativa.</p> <p></p> <p> Fonte: Example of a manual process. Adapted from: Google Cloud [4] </p>"},{"location":"machine-learning-engineering/mlops/#nivel-2-pipeline-de-ml-automatizado","title":"N\u00edvel 2 - Pipeline de ML Automatizado","text":"<p>O N\u00edvel 2 de automa\u00e7\u00e3o \u00e9 um n\u00edvel onde todo o processo de constru\u00e7\u00e3o \u00e0 valida\u00e7\u00e3o do modelo \u00e9 executado automaticamente conforme novos dados s\u00e3o disponibilizados ou o procedimento de retreino \u00e9 disparado (baseado em uma pol\u00edtica de agendamento ou threshold de performance). Assim, o objetivo e prover um processo treinamento cont\u00ednuo (CT, do ingl\u00eas continuous training) atrav\u00e9s da automatiza\u00e7\u00e3o de todo o pipeline de ML.</p> <p>Este n\u00edvel de automata\u00e7\u00e3o \u00e9 caracterizado por:</p> <ul> <li>Experimentos orquestrados<sup>3</sup></li> <li>Modelos em produ\u00e7\u00e3o que s\u00e3o continuamente atualizados automaticamente.</li> <li>Etapas de teste e deployment ocorrem manualmente.</li> </ul> <p></p> <p> Fonte: Example of an automated ML pipeline. Adapted from: Google Cloud [4] </p>"},{"location":"machine-learning-engineering/mlops/#nivel-3-pipeline-cicd","title":"N\u00edvel 3 - Pipeline CI/CD","text":"<p>No N\u00edvel 3 de automa\u00e7\u00e3o, todo o workflow ocorre automaticamente atrav\u00e9s de estrat\u00e9gias de CI/CD. Logo, diferente do anterior, as etapas de build, teste e deployment de cada um dos artefatos (dado, modelo e c\u00f3digo) tamb\u00e9m ocorrem automaticamente.</p> <p>Este n\u00edvel de automa\u00e7\u00e3o \u00e9 caracterizado por:</p> <ul> <li>Experimentos orquestrados</li> <li>Automa\u00e7\u00e3o completa de todo o pipeline de ML, incluindo build, teste e deployment de cada um dos artefatos relacionados ao pipeline.</li> </ul> <p></p> <p> Fonte: Example of a CI/CD pipeline. Adapted from: Google Cloud [4] </p>"},{"location":"machine-learning-engineering/mlops/#praticas-adicionais","title":"Pr\u00e1ticas Adicionais","text":""},{"location":"machine-learning-engineering/mlops/#teste-automatizados","title":"Teste Automatizados","text":"<p>Conforme a automa\u00e7\u00e3o do sistema de ML se torna mais sofisticada, as rotinas de teste devem acompanhar a evolu\u00e7\u00e3o do sistema e passarem a ser executadas automaticamente. Portanto, al\u00e9m dos testes unit\u00e1rios e de integra\u00e7\u00e3o, devemos incluir testes espec\u00edficos tanto para os modelos quanto dados.</p> <p>Por exemplo, checar se:</p> <ul> <li>O modelo (em desenvolvimento e produ\u00e7\u00e3o) n\u00e3o est\u00e1 enviesado.</li> <li>O modelo (em produ\u00e7\u00e3o) n\u00e3o est\u00e1 obsoleto.</li> <li>O dado segue os esquemas definidos.</li> </ul>"},{"location":"machine-learning-engineering/mlops/#monitoramento","title":"Monitoramento","text":"<p>Ap\u00f3s um modelo ir para produ\u00e7\u00e3o, ele precisa ser monitoramento a fim de garantir que funcione como o esperado. No contexto de pipelines de ML, o monitoramento \u00e9 um pr\u00e9-requisito para uma automa\u00e7\u00e3o apropriada. Em outras palavras, apenas atrav\u00e9s do monitoramento \u00e9 poss\u00edvel acompanhar a performance do modelo em produ\u00e7\u00e3o e automaticamente retrein\u00e1-lo quando ele se tornar obsoleto.</p> <p></p> <p> Fonte: ML Model Decay Monitoring and Retraining. Source: https://ml-ops.org/ </p>"},{"location":"machine-learning-engineering/mlops/#feature-stores","title":"Feature Stores","text":"<p>Uma Feature Store \u00e9 um servi\u00e7o centralizado de armazenamento e processamento de features atrav\u00e9s do qual as features s\u00e3o definidas, armazenadas e usadas tanto para o treinamento de modelos quanto para modelos em produ\u00e7\u00e3o. Desse modo, feature stores devem ser capazes de armazenar um grande volume de dados e fornecer acesso com baixa lat\u00eancia para as aplica\u00e7\u00f5es.</p> <p>Alguns benef\u00edcios de feature stores s\u00e3o:</p> <ul> <li>Reuso das features dispon\u00edveis atrav\u00e9s do compartilhamento do dado entre times e projetos (ao inv\u00e9s de recri\u00e1-las).</li> <li>Preven\u00e7\u00e3o de features semelhantes mas com defini\u00e7\u00f5es diferentes atrav\u00e9s da manuten\u00e7\u00e3o do pipeline de extra\u00e7\u00e3o de features e dos metadados relacionados.</li> <li>Disponibiliza\u00e7\u00e3o em escala e com baixa lat\u00eancia das feaures, principalmente para rotinas de retreinamento.</li> <li>Garantira de consist\u00eancia das features entre o processo de treinamento e deployment.</li> </ul>"},{"location":"machine-learning-engineering/mlops/#conclusao","title":"Conclus\u00e3o","text":"<p>Dado o crescente uso de ML em v\u00e1rios setores da ind\u00fastria e a necessidade por aplica\u00e7\u00f5es baseadas em ML manuten\u00edveis e escal\u00e1veis, a ado\u00e7\u00e3o da cultura de MLOps deve ser tornar um padr\u00e3o para todos aqueles que trabalham com IA ao longo dos pr\u00f3ximos anos. Afinal, MLOps tem se mostrado essencial em projetos de larga escala gra\u00e7as aos diversos benef\u00edcios indispens\u00e1veis que s\u00e3o gerados.</p>"},{"location":"machine-learning-engineering/mlops/#notas-e-comentarios","title":"Notas e Coment\u00e1rios","text":""},{"location":"machine-learning-engineering/mlops/#referencias","title":"Refer\u00eancias","text":"<ul> <li>Sculley, David, et al. \u201cHidden technical debt in machine learning systems.\u201d Advances in neural information processing systems 28 (2015): 2503\u20132511.</li> <li>\"ML-Ops.org.\" MLOps, ml-ops.org/.</li> <li>Burkov, Andriy. Machine learning engineering. True Positive Incorporated, 2020.</li> <li>\"MLOps: Continuous Delivery and Automation Pipelines in Machine Learning.\" Google Cloud, cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning</li> <li>Breck, Eric, et al. \"The ml test score: A rubric for ml production readiness and technical debt reduction.\" 2017 IEEE International Conference on Big Data (Big Data). IEEE, 2017.</li> </ul> <ol> <li> <p>Artefatos de ML s\u00e3o todos os (hiper)par\u00e2metros, scripts e dados de treinamento e teste utilizados para a constru\u00e7\u00e3o de um modelo.\u00a0\u21a9</p> </li> <li> <p>Dado compreende tanto o pipeline de dados quanto os dados utilizados para treinamento, valida\u00e7\u00e3o e teste. Modelo compreende todos os artefatos associados \u00e0 constru\u00e7\u00e3o do modelo. C\u00f3digo compreende tanto aos c\u00f3digos relacionados aos dados e modelo, quanto o c\u00f3digo-fonte da aplica\u00e7\u00e3o ao qual o modelo deve ser integrado.\u00a0\u21a9</p> </li> <li> <p>Experimentos orquestrados s\u00e3o aqueles cujas transi\u00e7\u00f5es entre cada etapa do experimento ocorre de maneira autom\u00e1tica e com um rastramento rigoroso.\u00a0\u21a9</p> </li> </ol>"},{"location":"machine-learning-engineering/model-serving/","title":"Model Serving","text":""},{"location":"machine-learning-engineering/model-serving/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>O model serving \u00e9 a etapa do fluxo de cria\u00e7\u00e3o de um modelo de machine learning (ML) que tem como objetivo disponibilizar o modelo para uso em conjunto de estrat\u00e9gias de implanta\u00e7\u00e3o (deployment).</p> <p>A forma como um modelo \u00e9 disponibilizado para a execu\u00e7\u00e3o de infer\u00eancias afeta diretamente, tanto intera\u00e7\u00e3o do usu\u00e1rio (ou aplica\u00e7\u00e3o) com ele, quanto seu desempenho e manutenibilidade. Por isso, ao dedicir qual estrat\u00e9gia utilizar, devemos levar em considera\u00e7\u00e3o fatores como:</p> <ul> <li>Padr\u00e3o de treinamento e infer\u00eancia (em lotes ou fluxo)</li> <li>Disponibilidade offline (poss\u00edvel fazer infer\u00eancias offline ou apenas online)</li> <li>Lat\u00eancia de rede (caso necess\u00e1rio envio de dados)</li> <li>Seguran\u00e7a de dados sens\u00edveis</li> <li>Padr\u00e3o de atualiza\u00e7\u00e3o do modelo</li> <li>Recursos de computa\u00e7\u00e3o dispon\u00edvel</li> </ul> <p>De fato, esses s\u00e3o os principais aspectos que nos levam a escolher uma estrat\u00e9gia de serving.</p> <p>Serving x Deployment</p> <p>Termos como model serving e model deployment s\u00e3o utilizados de forma intercambi\u00e1vel na interwebsv</p> <p>Para tornar as explica\u00e7\u00f5es mais claras e consistentes, vamos definir:</p> <ul> <li>Model serving. Forma que um modelo \u00e9 disponiblizado para infer\u00eancias</li> <li>Model deployment. Como a forma que o modelo \u00e9 disponiblizado \u00e9 realizada</li> </ul> <p>Por exemplo, podemos servir um modelo como um servi\u00e7o encapsulado em um container e cuja implanta\u00e7\u00e3o (ou deployment) se d\u00e1 atrav\u00e9s de um processo de CI/CD em Modo Can\u00e1rio.</p>"},{"location":"machine-learning-engineering/model-serving/#padroes-de-treinamento-e-inferencia","title":"Padr\u00f5es de Treinamento e Infer\u00eancia","text":"<p>Os padr\u00f5es de treinamento e infer\u00eancia s\u00e3o fatores fundamentais na decis\u00e3o de como um modelo deve ser disponibilizado, uma vez que h\u00e1 estrat\u00e9gias de serving que n\u00e3o suportam um determinado padr\u00e3o (ou combina\u00e7\u00e3o de padr\u00f5es) de treinamento e infer\u00eancia.</p>"},{"location":"machine-learning-engineering/model-serving/#padroes-de-retreinamento-de-modelos","title":"Padr\u00f5es de (re)Treinamento de Modelos","text":"<p>Considerando um cen\u00e1rio onde um modelo j\u00e1 est\u00e1 treinado e pronto para produ\u00e7\u00e3o, a preocupa\u00e7\u00e3o com seu padr\u00e3o de treinamento se d\u00e1 pela necessidade do seu retreinamento.</p> <p>Afinal, uma vez que o modelo est\u00e1 em produ\u00e7\u00e3o, ele \u00e9 exposto continuamente a dados novos do mundo real (e n\u00e3o apenas a uma amostra est\u00e1tica) e, consequentemente, torna-se obsoleto (fen\u00f4meno conhecido como model decay). Para que o modelo em produ\u00e7\u00e3o mantenha um bom desempenho na maior parte do tempo, ele deve ser retreinado com uma certa frequ\u00eancia.</p> <p>Modelos de ML s\u00e3o treinados de duas formas:</p> <ul> <li>Em Lotes (batches), conhecido como Aprendizado em Lotes ou Offline/Batch Learning.</li> <li>Incremental, conhecido como Aprendizado Incremental Online Learning.</li> </ul> <p>Logo, o retreinamento tamb\u00e9m pode acontecer em lotes ou de forma incremental.</p>"},{"location":"machine-learning-engineering/model-serving/#em-lotes","title":"Em Lotes","text":"<p>No retreinamento em lotes, um modelo \u00e9 retreinado ap\u00f3s um tempo consider\u00e1vel em produ\u00e7\u00e3o. Esse retreinamento pode acontecer em intervalos fixos (por exemplo, a cada 30 dias) ou quando um limite inferior de desempenho \u00e9 atingindo.</p> <p>O principal problema do retreinamento em lotes \u00e9 que, no caso de um intervalos fixo, a degrada\u00e7\u00e3o do modelo pode acontecer em diferentes velocidades, o que torna dif\u00edcil encontrar uma frequ\u00eancia de retreinamento ideal.</p> <p>Por outro lado, monitorar um modelo em produ\u00e7\u00e3o e retrein\u00e1-lo com base em alguma m\u00e9trica de desempenho, \u00e9 um processo consideravelmente complexo tanto pela defini\u00e7\u00e3o e c\u00e1lculo da m\u00e9trica em si, quanto pela necessidade de toda uma solu\u00e7\u00e3o de monitoramento.</p>"},{"location":"machine-learning-engineering/model-serving/#aprendizado-incremental","title":"Aprendizado Incremental","text":"<p>No \"aprendizado incremental\", o modelo \u00e9 treinado regularmente conforme novos dados s\u00e3o disponibilizados \u00e0 aplica\u00e7\u00e3o (e.g. real-time data streams). O retreinamento pode ocorrer em um \u00fanico dado novo ou pequenos grupos de dados, denominados mini-batches.</p> <p>O principal problema do Aprendizado Incremental \u00e9 que, quando em produ\u00e7\u00e3o, a entrada de dados ruins (e.g, ru\u00eddos) tende a prejudicar consideravalmente o desempenho do modelo.</p>"},{"location":"machine-learning-engineering/model-serving/#padroes-de-inferencia-do-modelo","title":"Padr\u00f5es de Infer\u00eancia do Modelo","text":"<p>Da mesma forma que treinados, modelos de ML podem ser dispostos para inferir dados de duas formas: Em Batches ou Sob-demanda.</p> <ul> <li>Na infer\u00eancia em batches, o modelo executa predi\u00e7\u00f5es sobre um \"grande\" volume de dados de uma vez e s\u00f3 ent\u00e3o retorna os resultados.</li> <li>Na infer\u00eancia em tempo real, as predi\u00e7\u00f5es s\u00e3o executadas sob-demanda para cada dado (ou pequenos conjuntos) de entrada e, em seguida, o resultado j\u00e1 \u00e9 retornado.</li> </ul> <p>O que \u00e9 grande?</p> <p>Grande \u00e9 relativo, n\u00e3o \u00e9 mesmo? De qualquer forma, no contexto de infer\u00eancia em batches significa que ao inv\u00e9s de executar infer\u00eancias sob-demanda em pequenos conjuntos de dados (e.g. 1 a ~300 inst\u00e2ncias), executa-se para v\u00e1rias entradas em conjunto (e.g., mais do que 500 ou 10 mil inst\u00e2ncias)</p>"},{"location":"machine-learning-engineering/model-serving/#padroes-de-model-serving","title":"Padr\u00f5es de Model Serving","text":"<p>Existem diversas abordagens para servir um modelo, cada um com suas vantagens e desvantagens. Alguns exemplos s\u00e3o:</p> <ul> <li>Modelo como Parte da Aplica\u00e7\u00e3o (Static Deployment)</li> <li>Model-as-a-Dependency (MaaD)</li> <li>Model-as-a-Service (MaaS)</li> <li>Serverless Servig/Deployment</li> <li>Hybrid-Serving (Federated Learning)</li> </ul>"},{"location":"machine-learning-engineering/model-serving/#referencias","title":"Refer\u00eancias","text":"<ul> <li>Three Levels of ML Software</li> <li>Machine Learning Engineering by Andriy Burkov</li> </ul>"},{"location":"machine-learning-engineering/model-serving/#appendix","title":"Appendix","text":""},{"location":"machine-learning-engineering/model-serving/#model-as-a-service-maas","title":"Model as a Service (MaaS)","text":""},{"location":"machine-learning-engineering/model-serving/#introducao_1","title":"Introdu\u00e7\u00e3o","text":"<p>A estrat\u00e9gia Model-as-a-Service \u00e9 a mais adotada para servir modelos de ML. Nela, o modelo \u00e9 abstra\u00eddo em um servi\u00e7o (tipicamente web) que recebe requisi\u00e7\u00f5es de infer\u00eancia. Ap\u00f3s executada as infer\u00eancias, o servi\u00e7o retorna os resultados ao requerente.</p> <p>Dependendo da implementa\u00e7\u00e3o, o servi\u00e7o pode receber tanto batches de dados para infer\u00eancia, quanto entradas individuais.</p> <p></p> <p> Imagem: Exemplo gen\u00e9rico da arquitetura Model-as-a-Service, onde um dispositivo faz requisi\u00e7\u00f5es de infer\u00eancia atrav\u00e9s de uma interface e ent\u00e3o recebe como resposta o resultado da predi\u00e7\u00e3o. </p> <p>Sobre Servi\u00e7os</p> <p>No contexto de arquitetura de software, um servi\u00e7o \u00e9 uma unidade de software auto-contida respons\u00e1vel por executar uma tarefa espec\u00edfica. Um servi\u00e7o deve conter todo o c\u00f3digo e dados necess\u00e1rios para executar sua tarefa, sendo implantando (geralmente) em um ambiente totalmente dedicado para si. Demais componentes do software (ou arquitetura) interagem com o servi\u00e7o atrav\u00e9s de uma API definida sobre protocolos de comunica\u00e7\u00e3o de rede, tais como REST APIs e HTTP/HTTPS.</p> <p>O prop\u00f3sito principal de um servi\u00e7o \u00e9 fornecer, ao sistema, acesso a um conjunto de funcionalidades, de modo que o servi\u00e7o provedor seja totalmente reutiliz\u00e1vel e independente do resto do sistema. Com isso, podemos desenvolver, construir e implantar o servi\u00e7o de forma totalmente desacoplada dos demais componentes.</p>"},{"location":"machine-learning-engineering/model-serving/#vantagens-x-desvantagens","title":"Vantagens x Desvantagens","text":"<p>Model as as Service \u00e9 uma estrat\u00e9gia adequada para a maioria das situa\u00e7\u00f5es. A principal ressalva \u00e9 que infer\u00eancias s\u00f3 estar\u00e3o dispon\u00edveis caso o usu\u00e1rio esteja online.</p>"},{"location":"machine-learning-engineering/model-serving/#vantagens","title":"Vantagens","text":"<p>As principais vantagens de se implantar modelos de ML como servi\u00e7os s\u00e3o:</p> <ul> <li>Integra\u00e7\u00e3o com o restante do sistema, tecnologias e processos extremamente simplificada.</li> <li>Gerenciamento do modelo simplificado.</li> </ul>"},{"location":"machine-learning-engineering/model-serving/#desvantagens","title":"Desvantagens","text":"<p>J\u00e1 os contras s\u00e3o:</p> <ul> <li>Necess\u00e1rio mais aplica\u00e7\u00f5es para gerenciar.</li> <li>N\u00e3o \u00e9 poss\u00edvel realizar infer\u00eancias offline.</li> <li>Lat\u00eancia de infer\u00eancia consideravelmente maior quanto comparado com infer\u00eancias offline, uma vez que os dados precisam ser enviados pela rede.</li> <li>Dados sens\u00edveis do usu\u00e1rio s\u00e3o enviados pela rede e executados em um dom\u00ednio externo ao dele.</li> </ul>"},{"location":"machine-learning-engineering/model-serving/#arquiteturas-baseadas-em-maquinas-virtuais-e-containers","title":"Arquiteturas baseadas em M\u00e1quinas Virtuais e Containers","text":"<p>Partindo de uma perspectiva de escalabilidade, podemos implantar os servi\u00e7os de predi\u00e7\u00e3o de duas formas principais: m\u00e1quinas virtuais ou containers.</p>"},{"location":"machine-learning-engineering/model-serving/#maquinas-virtuais","title":"M\u00e1quinas Virtuais","text":"<p>Com m\u00e1quinas virtuais (e.g. inst\u00e2ncias AWS EC2), usamos uma ou mais inst\u00e2ncias onde o servi\u00e7o web roda em paralelo (no caso de mais de uma inst\u00e2ncia).</p> <p>A necessidade de diversas inst\u00e2ncias se d\u00e1 quando h\u00e1 um grande volume de requisi\u00e7\u00f5es a ser atendido. Neste caso, tamb\u00e9m inclu\u00edmos um load balancer que ir\u00e1 receber as requisi\u00e7\u00f5es e redirecion\u00e1-las para a inst\u00e2ncia com maior disponibilidade.</p> <p>Note, entretanto, que a necessidade de virtualiza\u00e7\u00e3o prejudica consideravelmente a efici\u00eancia de uso dos recursos de cada inst\u00e2ncia.</p> <p></p> <p> Fonte: Machine Learning Engineering by Andriy Burkov (2020) </p>"},{"location":"machine-learning-engineering/model-serving/#containers","title":"Containers","text":"<p>Diferente de m\u00e1quinas virtuais, containers s\u00e3o consideravelmente mais eficientes no uso de recursos, tornando os gastos menores sem perda de desempenho (onde desempenho significa atender uma alta demanda de requisi\u00e7\u00f5es).</p> <p>Dessa forma, podemos usar um orquestrador de containers como o Kubernetes para gerenciar um conjunto de containers executando em uma ou mais m\u00e1quinas dentro de um cluster auto-escal\u00e1vel. Com essa estrat\u00e9gia, podemos reduzir o n\u00famero de r\u00e9plicas (ou seja, containers ativos em paralelo) para zero, quando n\u00e3o houver qualquer requisi\u00e7\u00e3o e aumentar para um n\u00famero suficientemente grande quando houver um grande volume de requisi\u00e7\u00f5es.</p> <p>No geral, a implanta\u00e7\u00e3o de servi\u00e7os de predi\u00e7\u00e3o em containers \u00e9 a mais indicada.</p> <p></p> <p> Fonte: Machine Learning Engineering by Andriy Burkov (2020) </p>"},{"location":"machine-learning-engineering/model-serving/#protocolos-de-comunicacao","title":"Protocolos de Comunica\u00e7\u00e3o","text":"<p>Assim como em qualquer servi\u00e7o convencional (aplica\u00e7\u00f5es web, banco de dados, etc), no Model as a Service (MaaS) a intera\u00e7\u00e3o com o modelo acontece atrav\u00e9s de APIs definidas sobre protocolos de comunica\u00e7\u00e3o em rede.</p> <p>As arquiteturas de API mais comuns s\u00e3o:</p> <ul> <li>REST (Representational State Transfer) com protocolo HTTP</li> <li>gRPC (Google Remote Procedure Call) com HTTP 2.0.</li> </ul>"},{"location":"machine-learning-engineering/model-serving/#exemplo","title":"Exemplo","text":"<p>A fim de solidificar o conhecimento, segue um exemplo pr\u00e1tico de serving (ou seria implanta\u00e7\u00e3o? ) de um modelo como um servi\u00e7o.</p>"},{"location":"machine-learning-engineering/model-serving/#model-as-a-service-com-fastapi-e-docker","title":"Model as a Service com FastAPI e Docker","text":"<p>O Python cont\u00e9m diversos pacotes como Flask, FastAPI e Uvicorn que nos permite definir facilmente uma API REST, tal como servidores altamente eficientes.</p> <p>Para detalhes sobre a implementa\u00e7\u00e3o de APIs em Python acesse a p\u00e1gina de cria\u00e7\u00e3o de APIs em Python:</p> <p>Supondo que j\u00e1 temos um modelo treinado (no caso, para a an\u00e1lise de sentimentos), a primeira etapa \u00e9 definir o endpoint de requisi\u00e7\u00e3o para infer\u00eancias e qual m\u00e9todo REST ser\u00e1 utilizado. Geralmente, nomeamos o endpoint como  <code>predict</code> ou <code>inference</code>  e usamos o m\u00e9todo <code>POST</code> (uma vez que com <code>POST</code> podemos definir o corpo da requisi\u00e7\u00e3o).</p> <p>Para definir o corpo da requisi\u00e7\u00e3o no FastAPI, utilizamos a estrat\u00e9gia de definir uma classe <code>UserRequest</code> derivada da classe <code>BaseModel</code> de <code>pydantic</code>.</p> <p>Em seguida, basta chamar o preditor para executar a infer\u00eancia e ent\u00e3o retornamos os resultados.</p> <pre><code># my_predictor.py\n\nfrom fastapi import FastAPI\nfrom textblob import TextBlob\n\napp = FastAPI(title=\"ML Model as a Service\")\n\nclass UserRequest(BaseModel):\n    sentence: str\n\n@app.post(\"/predict/\")\nasync def predict(user_request: UserRequest):\n    testimonial = TextBlob(user_request.sentence)\n    return {\n        \"result\": f\"Polarity is {testimonial.sentiment.polarity} and subjectivity is {testimonial.sentiment.subjectivity}\"  # type: ignore\n    }\n</code></pre> <p>Para executar as requisi\u00e7\u00f5es, iniciamos um servidor (com o uvicorn), fazendo a chamada no mesmo n\u00edvel do arquivo Python onde est\u00e1 definda a API:</p> <pre><code>$ uvicorn my_predictor:app --port &lt;port_number&gt;\n</code></pre> <p>Ent\u00e3o acessamos o endere\u00e7o <code>http://127.0.0.1:&lt;port_number&gt;/docs</code> e usamos a documenta\u00e7\u00e3o de API fornecida pela Swagger UI para enviar requisi\u00e7\u00f5es.</p> <p>Ainda, dado que o endpoint defindo \u00e9 da forma <code>http://&lt;ip_address&gt;:&lt;port_number&gt;/predict/?data=&lt;data&gt;</code>, podemos enviar requisi\u00e7\u00f5es de qualquer forma, incluindo via <code>curl</code></p> <pre><code>curl -X 'POST' \\\n  'http://localhost:8000/predict/' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"sentence\": \"Life is beautiful, enjoy it!\"\n}'\n</code></pre> <p>Podemos tamb\u00e9m definir o endpoint para receber requisi\u00e7\u00f5es <code>GET</code>, eliminando a passagem de dados via corpo de requisi\u00e7\u00e3o (embora isso seja mais uma vantagem do que desvantagem). </p> <pre><code># my_predictor.py\n\nfrom fastapi import FastAPI\nfrom textblob import TextBlob\n\napp = FastAPI(title=\"ML Model as a Service\")\n\n@app.get(\"/predict/\")\nasync def predict(sentence):\n    testimonial = TextBlob(sentence)\n    return {\n        \"result\": f\"Polarity is {testimonial.sentiment.polarity} and subjectivity is {testimonial.sentiment.subjectivity}\"  # type: ignore\n    }\n</code></pre> <p>Ent\u00e3o,  executamos as chamadas via web browser, por exemplo:</p> <pre><code>http://localhost:8000/predict/?sentence=%22Life%20is%20beautiful,%20enjoy%20it%22\n</code></pre>"},{"location":"machine-learning-engineering/model-serving/#conclusao","title":"Conclus\u00e3o","text":"<p>Este foi um exemplo pr\u00e1tico extremamente simples. No mundo real, h\u00e1 diversas outras considera\u00e7\u00f5es que devemos levar em conta durante o serving de um modelo como um servi\u00e7o, por exemplo:</p> <ul> <li>Quando um modelo \u00e9 atualizado, precisamos que isso se reflita no servi\u00e7o. Contudo, n\u00e3o podemos simplesmente desligar e religar um servi\u00e7o. Portanto, como atualizar os modelos para servi\u00e7os em produ\u00e7\u00e3o?</li> <li>\u00c9 muito poss\u00edvel que existam per\u00edodos em que a quantidade de requisi\u00e7\u00e3o \u00e9 grande o suficiente para derrubar o servi\u00e7o, tornando necess\u00e1rio o uso de um load balancer. Quando isso deve acontecer? Como deve ser o load balancer?</li> </ul> <p>Coment\u00e1rios sobre estes problemas ser\u00e3o inclu\u00eddos futuramente!</p> Interessado em mais exemplos?  <p>Caso queira ver mais exemplos de MaaS acesse o reposit\u00f3rio ahayasic/model-as-a-service-examples. Mas j\u00e1 adianto que n\u00e3o h\u00e1 uma explica\u00e7\u00e3o aprofundada para nenhum dos exemplos. Apenas c\u00f3digo \u00af\\_(\u30c4)_/\u00af</p>"},{"location":"machine-learning-engineering/model-serving/#modelo-como-parte-da-aplicacao-static-deployment","title":"Modelo como Parte da Aplica\u00e7\u00e3o (Static Deployment)","text":""},{"location":"machine-learning-engineering/model-serving/#introducao_2","title":"Introdu\u00e7\u00e3o","text":"<p>Nesta abordagem \\(-\\) que Andriy Burkov chama de Static Deployment \\(-\\) o modelo \u00e9 empacotado como parte da aplica\u00e7\u00e3o que ent\u00e3o \u00e9 instalada atrav\u00e9s de um arquivo de distribui\u00e7\u00e3o de aplica\u00e7\u00f5es, como, por exemplo: arquivo bin\u00e1rio execut\u00e1vel, JAR, APK, DEB, etc.</p>"},{"location":"machine-learning-engineering/model-serving/#vantagens-x-desvantagens_1","title":"Vantagens x Desvantagens","text":"<p>Static deployment \u00e9 uma estrat\u00e9gia adequada para modelos simples que precisam estar dispon\u00edveis o tempo todo (mesmo se o usu\u00e1rio estiver offline). Tamb\u00e9m \u00e9 adequado para infer\u00eancias em batches ou fluxo. Por\u00e9m, o retreinamento tende estar restrito a lotes, uma vez que \u00e9 necess\u00e1rio atualizar toda a aplica\u00e7\u00e3o para incluir uma nova vers\u00e3o do modelo.</p>"},{"location":"machine-learning-engineering/model-serving/#vantagens_1","title":"Vantagens","text":"<ul> <li>N\u00e3o \u00e9 preciso enviar dados do usu\u00e1rio para um servidor (ou qualquer recurso) externo ao dispositivo do usu\u00e1rio</li> <li>O modelo estar\u00e1 sempre dispon\u00edvel, mesmo se o usu\u00e1rio estiver offline (sem conex\u00e3o com a Internet)</li> <li>Caso seja um modelo simples, sem necessidade de computa\u00e7\u00f5es r\u00e1pidas ou pesadas, o tempo de infer\u00eancia \u00e9 muito mais r\u00e1pido na abordagem \"est\u00e1tico\" quando comparado com qualquer outra estrat\u00e9gia</li> </ul>"},{"location":"machine-learning-engineering/model-serving/#desvantagens_1","title":"Desvantagens","text":"<ul> <li>Para atualizar o modelo \u00e9 necess\u00e1rio atualizar toda a aplica\u00e7\u00e3o (ou seja, reconstruir o arquivo de distribui\u00e7\u00e3o da aplica\u00e7\u00e3o mesmo que apenas o modelo tenha sofrido altera\u00e7\u00f5es)</li> <li>Executar o monitoramento de performance do modelo \u00e9 extremamente dif\u00edcil</li> <li>Se a computa\u00e7\u00e3o do modelo for cara, execut\u00e1-la no dispositivo do usu\u00e1rio pode ser ineficiente ou prejudicar a experi\u00eancia do usu\u00e1rio</li> </ul>"},{"location":"machine-learning-engineering/model-serving/#model-as-a-dependency-maad","title":"Model as a Dependency (MaaD)","text":""},{"location":"machine-learning-engineering/model-serving/#introducao_3","title":"Introdu\u00e7\u00e3o","text":"<p>A estrat\u00e9gia MaaD \u00e9 bem parecida com static deployment. Contudo, ao inv\u00e9s de empacotarmos o modelo como parte da aplica\u00e7\u00e3o, ele \u00e9 definido como uma depend\u00eancia e empacotado de forma que seja poss\u00edvel atualiz\u00e1-lo individualmente.</p> <p>Por exemplo, podemos empacotar um modelo como:</p> <ul> <li>Um pacote instal\u00e1vel (e.g. um pacote Python) definido como uma depend\u00eancia da aplica\u00e7\u00e3o.</li> <li>Arquivo serializado que \u00e9 importado (e deserializado) pela aplica\u00e7\u00e3o durante sua inicializa\u00e7\u00e3o ou em tempo de execu\u00e7\u00e3o</li> <li>Arquivo com os par\u00e2metros do modelo que pode ser utilizado para atualiz\u00e1-lo.</li> </ul> <p>Neste cen\u00e1rio, para utilizar o modelo, a aplica\u00e7\u00e3o s\u00f3 precisa invocar um m\u00e9todo de infer\u00eancia deste.</p>"},{"location":"machine-learning-engineering/model-serving/#vantagens-x-desvantagens_2","title":"Vantagens x Desvantagens","text":"<p>Model as as Dependency \u00e9 uma estrat\u00e9gia adequada para modelos simples que precisam estar dispon\u00edveis o tempo todo (mesmo se o usu\u00e1rio estiver offline). Tamb\u00e9m \u00e9 adequado tanto para infer\u00eancias quanto retreinamento em batches ou fluxo.</p> <p>Por outro lado, modelos \"pesados\" devem ser evitados pelo alto custo de computa\u00e7\u00e3o no dispositivo do usu\u00e1rio.</p>"},{"location":"machine-learning-engineering/model-serving/#vantagens_2","title":"Vantagens","text":"<ul> <li>N\u00e3o \u00e9 preciso enviar dados do usu\u00e1rio para um servidor (ou qualquer recurso) externo ao dispositivo do usu\u00e1rio</li> <li>O modelo estar\u00e1 sempre dispon\u00edvel, mesmo se o usu\u00e1rio estiver offline.</li> <li>Caso seja um modelo simples, sem necessidade de computa\u00e7\u00f5es r\u00e1pidas ou pesadas, o tempo de infer\u00eancia \u00e9 muito mais r\u00e1pido quando comparado com qualquer outra estrat\u00e9gia</li> <li>O modelo pode ser atualizado sem a necessidade de atualizar toda a aplica\u00e7\u00e3o</li> </ul>"},{"location":"machine-learning-engineering/model-serving/#desvantagens_2","title":"Desvantagens","text":"<ul> <li>Executar o monitoramento de performance do modelo \u00e9 extremamente dif\u00edcil</li> <li>Se a computa\u00e7\u00e3o do modelo for cara, execut\u00e1-la no dispositivo do usu\u00e1rio pode ser ineficiente ou prejudicar a experi\u00eancia do usu\u00e1rio</li> <li>Dependendo da estrat\u00e9gia de empacotamento do modelo, t\u00e9cnicas de engenharia reversa podem ser aplicadas para manipular o resultado das infer\u00eancias</li> </ul>"},{"location":"machine-learning-engineering/model-serving/#exemplos","title":"Exemplos","text":""},{"location":"machine-learning-engineering/model-serving/#federated-learning","title":"Federated Learning","text":""},{"location":"machine-learning-engineering/model-serving/#introducao_4","title":"Introdu\u00e7\u00e3o","text":"<p>Hybrid-Serving, mais conhecido como Federated Learning \u00e9 uma forma relativamente nova, por\u00e9m, em alta, de servir modelos aos usu\u00e1rios, principalmente de dispositivos m\u00f3veis.</p> <p>Basicamente, trata-se de estrat\u00e9gia onde um modelo gen\u00e9rico \u00e9 disponibilizado para uma grande quantidade de usu\u00e1rios e, ent\u00e3o, cada usu\u00e1rio passa a ter um modelo espec\u00edfico para si que \u00e9 retreinado (ou especializado) em seus dados.</p> <p>Mais precisamente:</p> <ul> <li>H\u00e1 um modelo gen\u00e9rico no lado do servidor (ou server-side) pr\u00e9-treinado em dados do mundo real.</li> <li> <p>Tal modelo \u00e9 usado como ponto de partida para novos usu\u00e1rios da aplica\u00e7\u00e3o.</p> </li> <li> <p>Do lado dos usu\u00e1rios (ou user-side), h\u00e1 modelos especializados e \u00fanicos para cada usu\u00e1rio (que partem do modelo gen\u00e9rico no server-side), de forma que o retreinamento (i.e. especializa\u00e7\u00e3o) destes modelos para este usu\u00e1rio ocorre no dispositivo do usu\u00e1rio.</p> </li> <li>Uma vez especializados, os (hiper)par\u00e2metros de cada modelo s\u00e3o enviados para o servidor. Assim, o modelo do servidor \u00e9 ajustado a fim de que as tend\u00eancias reais de toda a comunidade de usu\u00e1rios sejam cobertas pelo modelo e, ent\u00e3o, este novo modelo passa a ser o novo modelo inicial para todos os usu\u00e1rios.</li> <li>Para que n\u00e3o haja desvantagens aos usu\u00e1rios, o processo de atualiza\u00e7\u00e3o dos modelos ocorre somente quando o aparelho est\u00e1 ocioso, conectado \u00e0 uma rede WiFi e carregando.</li> <li>Ainda, os testes s\u00e3o feitos nos dispositivos. Portanto, o modelo rec\u00e9m-adotado do servidor \u00e9 enviado aos dispositivos e testado quanto \u00e0 funcionalidade.</li> </ul> <p>A principal vantagem dessa abordagem \u00e9 que:</p> <ul> <li>Os dados pessoais necess\u00e1rios para o treinamento (e teste) nunca saem do dom\u00ednio do usuario. Enquanto que ainda assim \u00e9 poss\u00edvel atualizar os modelos com base nas tend\u00eancias da comunidade.</li> <li>Em outras palavras, \u00e9 poss\u00edvel treinar modelos de alta precis\u00e3o sem ter que armazenar toneladas de dados (provavelmente pessoais) na nuvem.</li> </ul> <p>Por\u00e9m, a grande desvantagem \u00e9 que a especializa\u00e7\u00e3o do modelo \u00e9 custosa para usu\u00e1rios. Afinal, os modelos de ML s\u00e3o desenvolvidos com conjuntos de dados grandes e homog\u00eaneos em um hardware poderoso.</p>"},{"location":"machine-learning-engineering/model-serving/#referencias_1","title":"Refer\u00eancias","text":"<ul> <li>Three Levels of ML Software</li> </ul>"},{"location":"python/","title":"Python","text":"<p>Tips and tricks about Python!</p>"},{"location":"python/importing-guide/","title":"Guia de Importa\u00e7\u00f5es em Python","text":"<p>Attention! Aten\u00e7\u00e3o!</p> <p>(en_US) This document is a translation and adaptation of the publication \"The Definitive Guide to Python import Statements\" by Chris Yeh.  I just made a few tweaks for my own use. For complete instructions, visit the original publication!</p> <p>(pt_BR) Este documento \u00e9 uma tradua\u00e7\u00e3o e adapta\u00e7\u00e3o da publica\u00e7\u00e3o \"The Definitive Guide to Python import Statements\" de Chris Yeh. Eu apenas fiz alguns ajustes para uso pr\u00f3prio. Para instru\u00e7\u00f5es completas, visite a publica\u00e7\u00e3o original!</p>"},{"location":"python/importing-guide/#terminologia","title":"Terminologia","text":"<ul> <li>M\u00f3dulo. Todo e qualquer arquivo <code>.py</code></li> <li>M\u00f3dulos Built-in: M\u00f3dulos padr\u00f5es da linguagem Python, geralmente escritos em C e traduzidos pra Python.</li> <li>Pacote. At\u00e9 o Python 3.3, qualquer diret\u00f3rio que contenha o arquivo <code>__init__.py</code>. A partir do Python 3.3, qualquer diret\u00f3rio que contenha ou n\u00e3o <code>__init__.py</code></li> <li>Objeto. Tudo do Python</li> </ul>"},{"location":"python/importing-guide/#estrutura-de-diretorios-de-exemplo","title":"Estrutura de Diret\u00f3rios (de Exemplo)","text":"<pre><code>test/                      # root folder\n    packA/                 # package packA\n        subA/              # subpackage subA\n            __init__.py\n            sa1.py\n            sa2.py\n        __init__.py\n        a1.py\n        a2.py\n    packB/                 # package packB (implicit namespace package)\n        b1.py\n        b2.py\n    math.py\n    random.py\n    other.py\n    start.py\n</code></pre>"},{"location":"python/importing-guide/#sobre-o-import","title":"Sobre o <code>import</code>","text":"<p>Quando executamos um <code>.py</code> (e.g. <code>python foo.py</code>), o interpretador do Python busca todos os m\u00f3dulos ou pacotes na seguinte ordem:</p> <ul> <li>M\u00f3dulos Built-in</li> <li>M\u00f3dulos e pacotes import\u00e1veis atrav\u00e9s dos caminhos definidos em <code>sys.path</code>.</li> </ul> <p>A vari\u00e1vel <code>sys.path</code> \u00e9 uma lista de caminhos a partir do qual podemos importar pacotes ao executarmos o <code>.py</code>. Assim, a <code>sys.path</code> sempre \u00e9 inicializada com os seguintes caminhos:</p> <ul> <li>Diret\u00f3rio corrente onde o <code>.py</code> est\u00e1 sendo executado.</li> <li>Diret\u00f3rios contidos na vari\u00e1vel de ambiente <code>PYTHONPATH</code>.</li> <li>Diret\u00f3rios padr\u00f5es do <code>sys.path</code>. Ou seja: Python Standard Library, pacotes instalados no ambiente virtual, etc.</li> </ul> <p>Logo, apenas os pacotes contidos nos caminhos de <code>sys.path</code> s\u00e3o import\u00e1veis.</p> <p>Al\u00e9m disso,</p> <ul> <li>Se o (interpretador) Python for invocado iterativamente, <code>sys.path[0]</code> \u00e9 uma string vazia, indicando que os m\u00f3dulos e pacotes devem ser pesquisados atrav\u00e9s do diret\u00f3rio corrente (a partir do interpretador foi chamado).</li> <li>J\u00e1 se o script for executado da forma <code>python foo.py</code>, ent\u00e3o <code>sys.path[0]</code> \u00e9 o caminho absoluto para o script em quest\u00e3o.</li> </ul> <p>Portanto, perceba que quando executamos um script Python, pouco importa qual \u00e9 o caminho do seu \"diret\u00f3rio de trabalho\", o que importa \u00e9 o caminho do script.</p> <p>Exemplo</p> <p>Supondo que a localiza\u00e7\u00e3o do nosso shell estiver em <code>test/</code>, se executarmos <code>python ./packA/subA/sa1.py</code>.</p> <ul> <li><code>sys.path[0]</code> ser\u00e1 <code>test/packA/subA/</code>, mas n\u00e3o <code>test/</code>.</li> </ul> <p>Al\u00e9m disso, <code>sys.path</code> \u00e9 compartilhada entre todos os m\u00f3dulos import\u00e1veis!</p> <p>Exemplo</p> <p>Suponha que fa\u00e7amos <code>user@user:~/test$ python start.py</code>, onde</p> <pre><code># test/start.py\nfrom packA import a1\n</code></pre> <pre><code># test/packA/a1.py\nimport sys\nprint(sys.path)\n</code></pre> <p>Com isso, <code>sys.path[0]</code> ser\u00e1 <code>test/</code> e, portanto, o m\u00f3dulo <code>a1.py</code> ser\u00e1 capaz de importar todo e qualquer m\u00f3dulo e pacote que parte de <code>test/</code>.</p>"},{"location":"python/importing-guide/#sobre-o-__init__py","title":"Sobre o <code>__init__.py</code>","text":"<p>O arquivo <code>__init__.py</code> tem dois pap\u00e9is:</p> <ul> <li>At\u00e9 o Python 3.3, <code>__init__.py</code> era respons\u00e1vel por tornar um pacote de scripts em um pacote de m\u00f3dulos import\u00e1veis.</li> <li>(Para toda e qualquer vers\u00e3o do Python) Respons\u00e1vel por executar o c\u00f3digo de inicializa\u00e7\u00e3o dos pacotes.</li> </ul> <p>Logo, para que os pacotes sejam considerados como m\u00f3dulos import\u00e1veis e inicializados apropriadamente (at\u00e9 o Python 3.3), devemos incluir o <code>__init__.py</code> no pacote em quest\u00e3o, como apresentado na estrutura de diret\u00f3rios.</p> <p>J\u00e1 para vers\u00f5es do Python superiores \u00e0 3.3, todos os diret\u00f3rios s\u00e3o considerados como pacotes, devido a ado\u00e7\u00e3o do pacotes de namespaces impl\u00edcitos.</p>"},{"location":"python/importing-guide/#inicializacao-dos-pacotes","title":"Inicializa\u00e7\u00e3o dos Pacotes","text":"<p>Sempre que um pacote (ou um dos seus m\u00f3dulos) for importado, o Python executa todo o c\u00f3digo contido em <code>__init__.py</code> (presente na ra\u00edz do pacote), caso exista. Com isso, todos os objetos import\u00e1veis definidos em <code>__init__.py</code> s\u00e3o considerados parte do namespace  do pacote<sup>1</sup>.</p> <p>Por exemplo, sejam:</p> <pre><code># test/packA/a1.py\ndef a1_func():\n    print(\"running a1_func()\")\n</code></pre> <pre><code># test/packA/__init__.py\n\n# this import makes a1_func directly accessible from packA.a1_func\nfrom packA.a1 import a1_func\n\nfrom packA_func():\n    print(\"running packA_func()\")\n</code></pre> <pre><code># test/start.py\nimport packA\n\npackA.packA_func()\npackA.a1_func()\npackA.a1.a1_func()\n</code></pre> <p>A sa\u00edda do comando <code>user@user:~/test$ python start.py</code> \u00e9:</p> <pre><code>running packA_func()\nrunning a1_func()\nrunning a1_func()\n</code></pre>"},{"location":"python/importing-guide/#sintaxe-para-importacao-de-objetos","title":"Sintaxe para Importa\u00e7\u00e3o de Objetos","text":"<p>Como j\u00e1 visto anteriormente, podemos executar importa\u00e7\u00f5es de 3 formas diferentes</p> <ol> <li> <p><code>import &lt;package or module&gt;</code></p> <p>Exemplo</p> <pre><code>import numpy\n</code></pre> </li> <li> <p><code>import &lt;package or module&gt; as alias</code></p> <p>Exemplo</p> <pre><code>import numpy as np\n</code></pre> </li> <li> <p><code>from &lt;package or module&gt; import &lt;inner&gt;</code></p> <p>Exemplo</p> <pre><code>from numpy import random\n</code></pre> </li> </ol> <p>No primeiro caso, ap\u00f3s a importa\u00e7\u00e3o, basta navegarmos entre os diferentes n\u00edveis do pacote atrav\u00e9s da nota\u00e7\u00e3o de ponto. Por exemplo, <code>pkgX.subpkgX.moduleX.X</code></p> <p>No segundo caso, definimos um \"apelido\" para o pacote ou m\u00f3dulo importado. Assim, podemos utiliz\u00e1-lo atrav\u00e9s do alias.</p> <p>J\u00e1 no terceiro caso, importamos um subpacote, m\u00f3dulo ou objeto mais interno do pacote principal.</p> <p>Note ainda que a nota\u00e7\u00e3o de ponto pode ser utilizada para importar diretamente os pacotes de interesse (em conjunto com <code>from</code>). Por exemplo <code>from pkgX.subpkgX.moduleX import X</code>.</p> <p>Por fim, lembre que apenas os objetos declarados no <code>__init__.py</code> de cada pacote (ou subpacote) s\u00e3o import\u00e1veis.</p>"},{"location":"python/importing-guide/#importacao-absoluta-vs-relativa","title":"Importa\u00e7\u00e3o Absoluta vs Relativa","text":"<p>Importa\u00e7\u00e3o absoluta \u00e9 aquela que usa o caminho absoluto (ou seja, a partir do diret\u00f3rio raiz do projeto) para o m\u00f3dulo que se deseja importar.</p> <p>Importa\u00e7\u00e3o relativa usa o caminho relativo (ou seja, a partir do m\u00f3dulo corrente) para o m\u00f3dulo que se deseja importar. Ainda, h\u00e1 dois tipos de importa\u00e7\u00e3o relativa.</p> <ul> <li>Importa\u00e7\u00e3o relativa expl\u00edcita. Importa\u00e7\u00f5es no formato <code>from .&lt;module or package&gt; import X</code>, onde <code>.</code> indica o diret\u00f3rio corrente; <code>..</code> indica o diret\u00f3rio anterior, etc.</li> <li>Importa\u00e7\u00e3o relativa impl\u00edcita.  Importa\u00e7\u00f5es definidas como se o diret\u00f3rio atual fosse parte de <code>sys.path</code>. Contudo, note que IMPORTA\u00c7\u00d5ES RELATIVAS IMPL\u00cdCITAS N\u00c3O EXISTEM NO PYTHON 3!!!</li> </ul> <p>Note ainda que, na importa\u00e7\u00e3o relativa conseguimos importar m\u00f3dulos apenas at\u00e9 o n\u00edvel a partir do qual estamos executando o <code>.py</code>.</p> <p>Portanto, \u00e9 recomendado o uso de importa\u00e7\u00e3o absoluta na grande maioria dos casos. Afinal, a importa\u00e7\u00e3o absoluta evita confus\u00f5es e inconsist\u00eancias. Al\u00e9m disso, scripts que usam importa\u00e7\u00e3o relativa n\u00e3o podem ser executados diretamente.</p>"},{"location":"python/importing-guide/#troubleshooting","title":"Troubleshooting","text":"<p>Uma das grandes vantagens da linguagem Python \u00e9 sua flexibilidade e durante o desenvolvimento de scripts tamb\u00e9m buscamos flexibilidade em como os m\u00f3dulos se relacionam entre si.</p> <p>Logo, seja para rodar um script diretamente ou importar um m\u00f3dulo dentro de outro, a flexibilidade \u00e9 importante. Por\u00e9m, o comportamento de importa\u00e7\u00e3o do Python produz algumas complica\u00e7\u00f5es especiais:</p> <ul> <li>Ao executarmos um script diretamente, \u00e9 imposs\u00edvel importar qualquer coisa de seu diret\u00f3rio-pai.</li> <li>O <code>sys.path</code> \u00e9 diferente para cada script, quando executados diretamente a partir da pasta em que cada um se encontra.</li> </ul>"},{"location":"python/importing-guide/#exemplo","title":"Exemplo","text":"<p>Vamos considerar a seguinte situa\u00e7\u00e3o. Dada nossa esturutra de diret\u00f3rios, queremos:</p> <ul> <li>Executar tanto o script <code>start.py</code> quanto <code>a2.py</code> diretamente.</li> <li>O <code>start.py</code> importa o m\u00f3dulo <code>a2.py</code>.</li> <li>O <code>a2.py</code> importa o m\u00f3dulo <code>sa2.py</code>.</li> </ul>"},{"location":"python/importing-guide/#problema","title":"Problema","text":"<p>Ao executarmos <code>user@user:~test/$ python start.py</code>, o <code>sys.path</code> ter\u00e1 <code>test/</code> e as declara\u00e7\u00f5es de importa\u00e7\u00f5es ser\u00e3o:</p> <pre><code># test/start.py\nfrom packA import a2\n</code></pre> <pre><code># test/packA/a2.py\nfrom packA.subA import sa2\n</code></pre> <p>Por\u00e9m, ao executarmos <code>user@user:~test/packA$ python a2.py</code>, o <code>sys.path</code> ter\u00e1 <code>test/packA/</code>. Logo:</p> <ul> <li>A declara\u00e7\u00e3o de importa\u00e7\u00e3o <code>from packA.subA import sa2</code> n\u00e3o ir\u00e1 funcionar, pois <code>packA</code> n\u00e3o \u00e9 um diret\u00f3rio contido em <code>test/packA/</code>.</li> <li>Se trocarmos a declara\u00e7\u00e3o para <code>from subA import sa2</code>, ser\u00e1 poss\u00edvel executar <code>python a2.py</code>, mas imposs\u00edvel executar <code>python start.py</code>, pois n\u00e3o h\u00e1 <code>subA/</code> em <code>sys.path</code>.</li> </ul>"},{"location":"python/importing-guide/#solucoes-workarounds","title":"Solu\u00e7\u00f5es (Workarounds)","text":"<p>Consideando estes problemas, podemos recorrer \u00e0s seguintes estrat\u00e9gias:</p> <ol> <li> <p>Use a importa\u00e7\u00e3o absoluta (sempre partindo da ra\u00edz, no caso <code>test/</code>) em todos os m\u00f3dulos, executando m\u00f3dulos mais internos a partir da ra\u00edz (recomendado).</p> <ul> <li>Com isso, voc\u00ea ser\u00e1 capaz de executar o <code>start.py</code> diretamente.</li> <li> <p>Para executar o <code>a2.py</code> diretamente, execute-o como um m\u00f3dulo:</p> <ul> <li>V\u00e1 para a raiz do diret\u00f3rio de trabalho</li> <li>Execute <code>a2.py</code> como um m\u00f3dulo importado</li> </ul> <pre><code>python -m packA.a2\n</code></pre> </li> </ul> </li> <li> <p>Use a importa\u00e7\u00e3o absoluta (sempre partindo da ra\u00edz, no caso <code>test/</code>) em todos os m\u00f3dulos, sendo o diret\u00f3rio ra\u00edz adicionado na <code>sys.path</code>.</p> <ul> <li>Com isso, voc\u00ea ser\u00e1 capaz de executar o <code>start.py</code> diretamente.</li> <li>Para executar o <code>a2.py</code>, adicione o diret\u00f3rio ra\u00edz <code>test/</code> antes da importa\u00e7\u00e3o de <code>sa2.py</code></li> </ul> <pre><code>import os, sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))\n\nfrom packA.subA import sa2\n</code></pre> <p>Nota</p> <p>Este m\u00e9todo funciona no geral. Contudo, h\u00e1 situa\u00e7\u00f5es em que a vari\u00e1vel <code>__file__</code> pode estar incorreta. Neste caso, precisamos de uma solu\u00e7\u00e3o mais robusta. Acesse essa resposta no StackOverflow para mais instru\u00e7\u00f5es.</p> </li> <li> <p>Instale o pacote em modo de desenvolvimento no ambiente virtual. Com isso, a ra\u00edz do diret\u00f3rio sempre estar\u00e1 presente no <code>sys.path</code> como um pacote instalado (recomendado).</p> <p>Dica</p> <p>Para informa\u00e7\u00f5es sobre cria\u00e7\u00e3o e instala\u00e7\u00e3o de pacotes, veja Criando Pacotes e M\u00f3dulos.</p> </li> </ol>"},{"location":"python/importing-guide/#referencias","title":"Refer\u00eancias","text":"<ul> <li>The Definitive Guide to Python import Statements</li> </ul> <ol> <li> <p>Isso significa que podemos import\u00e1-los atrav\u00e9s da sintaxe <code>pkgname.&lt;obj_to_import&gt;</code> \u21a9</p> </li> </ol>"},{"location":"python/linting/","title":"Linting","text":""},{"location":"python/linting/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Durante os momentos de escrita de c\u00f3digo podemos cometer uma s\u00e9rie de erros:</p> <ul> <li>Erros de sintaxe.</li> <li>Erros l\u00f3gicos que prejudicam a seguran\u00e7a da aplica\u00e7\u00e3o.</li> <li>Erros de estiliza\u00e7\u00e3o do c\u00f3digo.</li> </ul> <p>Por\u00e9m, tais erros s\u00e3o normais de acontecerem e, mesmo com muito esfor\u00e7o, \u00e9 dif\u00edcil encontrarmos todos eles.</p> <p>Portanto, utilizamos ferramentas capazes de analisar e detectar problemas em nossos c\u00f3digos. Essas ferramentas, chamadas de linters, s\u00e3o capazes de checar:</p> <ul> <li>Erros l\u00f3gicos (Logical Lint)<ul> <li>Erros de sintaxe.</li> <li>Peda\u00e7os de c\u00f3digo com resultados indesejados (bugs).</li> <li>Peda\u00e7os de c\u00f3digo com falhas de seguran\u00e7a.</li> </ul> </li> <li>Erros de estiliza\u00e7\u00e3o (Stylistic Lint)<ul> <li>C\u00f3digo n\u00e3o alinhado com o Guia de Estilo definido.</li> </ul> </li> </ul> <p>H\u00e1 tamb\u00e9m outras ferramentas que podemos usar em complemento aos linters que nos ajudam a melhorar a qualidade do c\u00f3digo, como \u00e9 o caso de formatadores autom\u00e1ticos de c\u00f3digo (code formatters).</p> <p>Nem tudo \u00e9 o que parece</p> <p>Na maioria dos casos, os linters que usamos s\u00e3o, na verdade, uma combina\u00e7\u00e3o de v\u00e1rios linters que detectam problemas espec\u00edficos.</p>"},{"location":"python/linting/#linting-flake8","title":"Linting: flake8","text":"<p>A linguagem de program\u00e7\u00e3o Python possui v\u00e1rios linters. Um famoso, \u00e9 o flake8, capaz de detectar erros l\u00f3gicos e de estiliza\u00e7\u00e3o (PEP8), sendo uma combina\u00e7\u00e3o dos linters:</p> <ul> <li>PyFlakes</li> <li>pycodestyle (PEP8)</li> <li>Mccabe</li> </ul> <p>A instala\u00e7\u00e3o do flake8 \u00e9 simples, basta executarmos:</p> <pre><code>pip install flake8\n</code></pre> <p>Se o seu ambiente de desenvolvimento possuir suporte para linting autom\u00e1tico (e.g. Visual Studio Code), os erros ser\u00e3o sinalizados no pr\u00f3prio ambiente.</p> <p>Caso contr\u00e1rio, podemos executar o linting especificamente nos arquivos desejados</p> <pre><code>flake8 path/to/files/\n</code></pre> <p>Ainda, podem haver erros ou avisos que gostar\u00edamos de omitir por ser um falso-positivo ou que ao inv\u00e9s de contribuir para a melhoria da qualidade do c\u00f3digo, age ao contr\u00e1rio.</p> <p>Nesse caso, basta adicionamos a flag <code>--ignore=</code> seguido dos c\u00f3digos de erros que desejamos ignorar.</p> <pre><code>flake8 --ignore=E1,E23,W503 path/to/files\n</code></pre> <p>Se o objetivo for ignorar apenas linhas espec\u00edficas, basta incluirmos a flag <code># noqa:</code> seguida dos c\u00f3digos de erro.</p> <pre><code>example = lambda: 'example'  # noqa: E731,E123\n</code></pre> <p>Dica</p> <p>Para ignorar todo e qualquer tipo de erro (ou aviso), use apenas <code># noqa</code></p>"},{"location":"python/linting/#arquivos-de-configuracao","title":"Arquivos de Configura\u00e7\u00e3o","text":"<p>A fim de garantir que todas as pessoas que estiverem trabalhando no mesmo projeto sigam as mesmas diretrizes, podemos definir as configura\u00e7\u00f5es do flake8 em arquivos de configura\u00e7\u00e3o na ra\u00edz do projeto.</p> <p>As configura\u00e7\u00f5es podem estar tanto em um arquivo <code>.flake8</code> ou em <code>setup.cfg</code> ou <code>tox.ini</code>.</p> <p>Basta ent\u00e3o adicionar a chave <code>[flake8]</code>seguida das configura\u00e7\u00f5es desejadas. Por exemplo:</p> <pre><code>[flake8]\nignore = D203\nexclude =\n    .git,\n    __pycache__,\n    docs/source/conf.py,\n    old,\n    build,\n    dist\nmax-complexity = 10\n</code></pre>"},{"location":"python/linting/#going-deeper","title":"Going Deeper","text":"<p>No caso de linguagens tipadas dinamicamente, como Python, mas que suportam typing annotation, podemos usar ferramentas adicionais para garantir que os tipos, quando anotados, se comportam como o esperado.</p> <p>\u00c9 o caso da ferramenta mypy.</p>"},{"location":"python/linting/#typing-hinting-com-mypy","title":"Typing hinting com mypy","text":"<p>Python \u00e9 uma linguagem dinamicamente tipada, logo as tipagens ocorrem em tempo de execu\u00e7\u00e3o (ou seja, conforme o c\u00f3digo \u00e9 executado).</p> <p>Por\u00e9m, h\u00e1 situa\u00e7\u00f5es em que queremos garantir que os tipos da vari\u00e1veis sejam respeitados, como, por exemplo, na passagem de par\u00e2metros para uma fun\u00e7\u00e3o ou no retorno desta.</p> <p>A ferramenta mypy \u00e9 uma type checker  capaz de analisar e validar se as anota\u00e7\u00f5es de tipo s\u00e3o respeitadas com base nas PEP 484 e 526.</p> <p>Para instalar o <code>mypy</code>, basta rodar</p> <pre><code>pip install mypy\n</code></pre> <p>Ent\u00e3o, para fazer a valida\u00e7\u00e3o das anota\u00e7\u00f5es de tipo, executamos:</p> <pre><code>mypy script.py\n</code></pre> <p>Nota</p> <p>O mypy i\u0155a executar toda a an\u00e1lise de forma est\u00e1tica. Isto \u00e9, assim como um linter, n\u00e3o ser\u00e1 executada qualquer linha de c\u00f3digo.</p>"},{"location":"python/linting/#typing-hinting-com-pyright","title":"Typing hinting com Pyright","text":"<p>Pyright \u00e9 um type checker (assim como mypy) com foco em analisar grandes bases de c\u00f3digo Python, especialmente em modo \"watch\". As an\u00e1lises s\u00e3o feitas de forma incremental conforme os arquivos s\u00e3o modificados, possibilitando que grandes conjuntos de arquivos sejam verificados rapidamente, enquanto escritos.</p> <p>Ainda, o Pyright \u00e9 uma ferramenta integrada ao Pylance que, por sua vez, faz parte do ecossistema de extens\u00f5es Python da Microsoft para o Visual Studio Code.</p> <p>Para ativ\u00e1-lo no VSCode, basta abrir o painel de configura\u00e7\u00f5es e procurar por <code>type checking</code>. O resultado deve ser:</p> <pre><code>Python \u203a Analysis: Type Checking Mode\nDefines the default rule set for type checking.\n</code></pre> <p>As op\u00e7\u00f5es de regras s\u00e3o:</p> <ul> <li><code>basic</code>. Checagem simples, sem muitas restri\u00e7\u00f5es.</li> <li><code>strict</code>. Checagem completa, com diversas restri\u00e7\u00f5es e alta sensibilidade a poss\u00edveis erros.</li> </ul> <p>Tamb\u00e9m \u00e9 poss\u00edvel configurar o pyright via arquivo de configura\u00e7\u00f5es (e.g. <code>settings.json</code>). Neste caso, basta adicionar ao arquivo:</p> <pre><code>{\n  \"python.analysis.typeCheckingMode\": \"basic\"  # or `strict`\n}\n</code></pre> <p>Typing hints podem ser legais?</p> <p>Para um material pr\u00e1tico sobre o uso de typing hints em Python, veja a se\u00e7\u00e3o Typing Hints</p>"},{"location":"python/linting/#linting-durante-testes","title":"Linting durante testes","text":"<p>A fim de garantir que toda adi\u00e7\u00e3o ou altera\u00e7\u00e3o de c\u00f3digo no projeto respeite as conven\u00e7\u00f5es de estilo e padr\u00f5es de qualidade, podemos incluir a valida\u00e7\u00e3o de linters como uma etapa de teste (que \u00e9 executada durante um pipeline de CI/CD) ou mesmo antes de um commit ser feito (considerando que h\u00e1 o uso de Git).</p> <p>Para tal, podemos usar a ferramenta pre-commit, GitHub Actions ou GitLab CI/CD.</p>"}]}